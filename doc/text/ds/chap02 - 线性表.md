# 线性表

**线性表**是指相同类型的有限个数据组成的序列。在这本笔记中采用的是清华大学教材的分法，将线性表包括**向量**（vector）和**列表**（list）两种形式，分别对应C++ STL里的`vector`和`list`。

> 在另一些教材中，这两个词被称为**顺序表**和**链表**，分别对应Java里的`ArrayList`和`LinkedList`。向量（顺序表）和列表（链表）这两对概念通常可以混用。

向量和列表代表着两种最基本的数据结构组织形式：**顺序结构**和**链式结构**。本章在分别介绍这两种数据结构之后，将会进一步介绍这两种结构级联得到的**分块结构**。

## 向量

> 中国革命必须分为两个步骤。第一步，改变这个殖民地、半殖民地、半封建的社会形态，使之变成一个独立的民主主义的社会。第二步，使革命向前发展，建立一个社会主义的社会。
>
> ——Chairman Mao

**向量**（vector）是一个基于**数组**（array）的数据结构，因此向量在内存中占据的是一段连续的空间。

### 向量和数组

**元素**（element）代表数据结构中的单个数据单元，它可能是基本数据类型、结构体、函数等。一个数据结构中的元素具有统一的类型。所有数据结构都是由元素构成的。

**线性表**是元素成线性排列的表。作为一种基于数组的线性表，<u>向量的元素次序和数组的元素次序相同</u>。如果一个向量`V`基于数组`A[0:m]`构建，那么向量`V`的第`i`个元素就是`A[i]`。

> 说向量元素的**物理次序**和**逻辑次序**相同是不妥的，尽管这种说法可能能帮助您理解向量，但可能会引起新的混乱。
>
> 在《计算机组成原理》和《操作系统》学科中您将会看到，<u>“物理”和“逻辑”这两个相对的名词一般用于**实际内存**和**虚拟内存**的场合</u>。数组在虚拟内存上的逻辑地址总是连续的，但在实际内存上的物理地址受操作系统调度影响，可能由多个不连续的部分组成。所以用“物理次序”这样一个有歧义的名词是不妥的。

需要注意的是，尽管向量和它基于的数组在<u>元素次序上相同</u>，但在<u>元素数量上是不一定相同</u>的。数组的元素数量总是恒定的，而向量的元素数量是<u>运行时可变的</u>。

记一个长度为`n`的向量为`V[0:n] = { V[0], V[1], ..., V[n-1] }`，则`n`称为向量`V`的**规模**（size），而称为向量`V`分配的内存空间可以容纳的元素数量`m`（即它所基于的数组的规模）为向量`V`的**容量**（capacity）。

> 一个向量的<u>规模必定不大于容量</u>。在不超过容量的前提下，向量的规模可以灵活变化，从而赋予了它比数组更高的灵活性。

对于向量中的每一个元素`V[i]`来说，它前面的元素称为它的**前驱**（predecessor），它后面的元素称为它的**后继**（successor），特别地，和它位置相邻的前驱，也就是**直接前驱**为`V[i-1]`，相应地，**直接后继**为`V[i+1]`。所有的前驱构成了**前缀**（prefix），也就是`V[0:i]`；所有的后继构成了**后缀**（suffix），也就是`V[i+1:n]`。

### 循秩访问

在本笔记中介绍的各种数据结构，将从一个非常小的基础模板开始逐步添加功能。您可以跟随笔记的正文自己实现模板，或参考配套代码中的模板。

> 为了让模板更有用，配套代码里还实现了一些和《数据结构》知识关系不大的函数。这些内容在笔记正文中不会介绍。

在上一小节中已经介绍过向量的基本元素：用于存放数据的数组，以及规模、容量两个基本属性。据此，可以写出一个基本的框架如下。

```c++
template <typename T>
class Vector {
    T* _data;      // 向量所基于的数组
    int _capacity; // 向量的容量
    int _size;     // 向量的规模
public:
    int capacity() { return _capacity; }
    int size() { return _size; }
};
```

> 这里将规模和容量都定义成了**私有**（private）变量，并提供了公共的`getter`方法。因为向量外的代码不应该直接修改它们。

对于向量中的元素，其访问方式称为**循秩访问**。称元素`V[i]`在向量`V`中的序号，也就是`i`，为它的**秩**（rank）。对于建立在数组`A`上的向量`V`，因为`V`和`A`的元素次序是一致的，所以`V[i] = A[i] = *(A+i)`。因此，只要知道一个元素的秩，就可以在$O(1)$的时间内访问该元素。

```c++
typedef int Rank;

template <typename T>
T& Vector<T>::operator[](Rank index) {
    return _data[index];
}
```

> 如果`index`超出了`capacity`，会发生数组越界，可能引起**段错误**（segmentation fault）。如果追求稳健性，应当增加一个`index < capacity`的判断。但为效率起见，这里舍弃了这一判断。
>
> 后文中同样会舍弃一些稳健性的考量。

### 装填因子

设向量的容量为`m`，规模为`n`，则称比值`n/m`为**装填因子**（load factor）。正常情况下，这是一个`[0,1]`之间的数。装填因子是衡量向量效率的重要指标。

<u>如果装填因子过小</u>，则会造成内存浪费：申请了巨大的数组，但其中只有少量的单元被向量中的元素用到，其他单元都被闲置了。

<u>如果装填因子过大</u>（超过1），则会引发数组越界，造成段错误。

> 刚开始的时候，装填因子一定是在`[0,1]`之间的。
>
> 但因为数组的容量`m`是固定的，而向量的规模`n`是动态的，所以一开始分配的`m`可能后来会不够用，从而产生装填因子大于1的问题。

为了让装填因子保持在一个合理的范围，需要允许动态地改变容量`m`的值。令m增大的操作称为**扩容**，令m减小的操作称为**缩容**。

### 改变向量的容量

首先来看向量扩容。

```c++
// 问题2.1 - 向量扩容
// 分析合适的方式让向量扩容
```

这就不是上一章那种简单的算法问题了，需要逐步分析。

首先，直接在原向量所占空间后面增加“一条尾巴”是不现实的，因为原向量所占空间后面的地址，可能已经被分配给了其他变量。因此，合适的做法是<u>重新建立一个更大的数组，将原向量的数据复制到新数组中，再把原数组的空间释放掉</u>。

> 注意这个操作对于缩容也是一样的，只是缩容是建立了一个更小的数组。

```c++
// 算法2.1A
template <typename T>
void Vector<T>::set_capacity(int new_capacity) {
    T* old = _data;
    _data = new T[_capacity = new_capacity];
    arrayCopy(_data, old, _size);
    delete[] old;
}
```

> 这里`arrayCopy(T* dst, T* src, int size)`函数用来将大小为`size`的数组从`src`移动到`dst`位置。

设`new_capacity`为`M`，则上述**算法2.1A**的时间和空间复杂度均为$O(M)$。

### 等差扩容和等比扩容

**算法2.1A**仍然没有解决关键问题：在实际使用中，用户不一定知道应该扩容到多少：如果新的容量`M`比较小，就可能再次发生数组越界，就需要再次调用`set_capacity`函数扩容，<u>消耗时间</u>；如果`M`比较大，则装填因子太低，<u>浪费空间</u>。

<u>所以，向量的设计者应该提供一个合理的扩容规则</u>。如果用户知道怎么扩容当然最好，但当用户不知道应该扩容到多少的时候，这个<u>合理的扩容规则</u>给了他们一个备选项。只要选择按照向量设计者提供的规则来扩容，使得时间和空间效率都不会太低。

那么应该如何设计扩容规则呢？

第一种方法是<u>按照等差数列扩容</u>，规定一个公差`d`，每次扩容都让容量`+d`（**固定扩容**），也就是从`m`变成`m+d`；第二种方法是<u>按照等比数列扩容</u>，规定一个公比`q`，每次扩容都让容量`*q`（**比例扩容**），也就是从`m`变成`qm`。

```c++
// 算法2.1B - 基本框架
template <typename T>
void Vector<T>::expand(function<int(int)> strategy) {
    set_capacity(strategy(_capacity));
}

// 算法2.1B1 - 固定扩容
template<typename T>
void expandByAP(const Vector<T>& V, int d) {
    V.expand([=](int m) -> { return m + d; });
}

// 算法2.1B2 - 比例扩容
template<typename T>
void expandByGP(const Vector<T>& V, double q) {
    V.expand([=](int m) -> { return (int)(m * q); });
}
```

> 这里允许**比例因子**`q`不是整数。实际使用时，如果比例因子是整数，则应当声明`q`为`int`类型以加快运算速度。特别地，如果`q`是2的幂次，可以写成移位计算的形式进一步加快运算速度。

很显然，在`strategy`函数只进行了简单的加（或乘）运算的情况下，`expand`只进行了一次`set_capacity`的操作，其效率和单次的`set_capacity`基本一致。

那么，<u>固定扩容和比例扩容</u>这两种扩容方式哪种更好？它们都只调用了一次`expand`，所以它们的效率是一样的吗？

显然不是的。设计**算法2.1B1**和**算法2.1B2**的原理，是按照等差或等比“<u>*数列*</u>”扩容，而不是“<u>*一次*</u>”扩容。所以评价这两种扩容规则的标准，不是<u>进行一次扩容</u>的效率或<u>进行一次扩容</u>后的装填因子，而是比较<u>一系列扩容</u>操作的总体效率和在这<u>一系列扩容</u>操作中的平均装填因子。用已有的复杂度分析工具不足以对这两种策略的效率进行准确评价。

### 分摊复杂度分析

为了对**一系列操作**进行分析，需要引入新的复杂度分析标准。

一般地，假设$O_1, O_2, \dots, O_n$是连续进行的`n`次操作，则当$n\to\infty$，这`n`次<u>连续操作所用时间的平均值的复杂度</u>，称为这一操作的**分摊复杂度**，对分摊复杂度的分析称为**分摊分析**。分摊分析的原则之一是：<u>使用相同效果的操作序列</u>。所以，要比较**算法2.1B1**和**算法2.1B2**，不应该把每次操作取为"进行一次扩容"（因为两种方法扩容量不一样)，而应该取为“向量`V`的规模增加`1`”。连续进行`n`次操作，就可以考虑向量`V`的规模从`0`增长为`n`的过程。

**算法2.1B1**中，容量依次被扩充为$d,2d,3d,\dots,n$，共进行$n/d$次扩容。

因此，分摊复杂度为：
$$
T(n) = \frac{d + 2d + 3d + \dots + n}n = \frac{(n/d)\cdot d + (n/d)(n/d-1)/2\cdot d}n = \frac{n/d+1}2 = O\left(\frac nd\right)
$$
另一方面，进行k次扩容之后的装填因子至少为$\frac{kd}{(k+1)d}=\frac{k}{k+1}$，当$k\to\infty$时，装填因子趋于**100%**。

**算法2.1B2**中，容量被依次扩充为$q,q^2,q^3,\dots,n$，共进行$\log _q n$次扩容。

因此，分摊复杂度为：
$$
T(n)=\frac{q+q^2+q^3+\dots+n}n=\frac{q\cdot\frac{1-n}{1-q}}n=O(1)
$$
另一方面，装填因子不断在$[\frac 1q,1]$之间线性增长，平均装填因子为$\frac{1+q}{2q}$。可以看出，不管怎样选择`q`，对分摊复杂度都没有影响，而更小的`q`能够带来更大的平均装填因子。因此，在实际的向量扩容过程中，总是选择`q = 2`，这时平均装填因子为**75%**。

从上面的分摊分析中可以看出，**算法2.1B1**尽管能够带来更高的装填因子，但时间效率略有不足；而**算法2.1B2**则在保证装填因子不太低的情况下，时间效率非常优秀。

所以，通常向量扩容会选择**算法2.1B2**的规则。

> 不过，**算法2.1B2**也有其劣势，就是容量越大，装填因子不高带来的空间浪费愈发明显，所以有些对空间要求较高的情况下，也采用二者相结合的方式：<u>在容量比较小时加倍扩容、在容量比较大的时候固定扩容</u>，在《计算机网络原理》的学习中您将看到二者相结合的例子。

### 缩容

介绍完扩容，缩容的方法也呼之欲出了：无非是**固定缩容**或者**减半缩容**。您可以自己完成这两个算法。

缩容的重要性远不如扩容，因为缩容降低`m`之后，如果`n`又扩大回去了，就又要扩容回去，这一缩一扩浪费了不少时间，而发挥的价值甚微（除非在这段缩、扩之间的时间里，释放出的内存另有他用）。为了避免一缩一扩浪费时间，通常会规定一个**缩容阈值**，当装填因子小于这个阈值的时候才会缩容。

对于常用的<u>比例扩容+比例缩容</u>的策略，您可以自己证明，只要<u>缩容阈值<50%</u>，就可以保证对于任意的操作序列（每个操作是`n`加一或减一），扩容和缩容的<u>综合分摊复杂度</u>为$O(1)$。在实际的向量中，为了方便计算，缩容阈值通常使用25%、12.5%等数值，取为0（**禁止缩容**）也是一个常见的策略。

### 插入单个元素

对于任何数据结构，都有三种基本的操作：

* **插入**（insert）：向数据结构中插入一个元素。

* **查找**（find）：查找一个元素在数据结构中的位置。

* **删除**（delete）：从数据结构中移除一个元素。

  > 因为`delete`是许多语言的关键字，所以在编程的时候常用`remove`代替它。

下面就以向量为例，分别介绍这三种基本的操作。

```c++
// 问题2.2 - 向量插入元素
// 给定：向量V[0:n]
// 输入：待插入的元素e，目标的秩r
// 要求：将元素e插入到V[r]的位置上
```

首先讨论插入。

要将待插入的元素`e`插入到`V[r]`，那么可以将原来的向量`V[0:n]`分成`V[0:r]`和`V[r:n]`两部分。

* 插入之前，向量是`V[0:r]`，`V[r:n]`。
* 插入之后，向量是`V[0:r]`，`e`，`V[r:n]`。

可以发现，在插入的前后，前一段`V[0:r]`的位置是不变的，而后一段`V[r:n]`需要整体向后移动1个单元的位置。据此，可以设计下面的算法。

```c++
// 算法2.2A
template <typename T>
void Vector<T>::insert(T e, Rank r) {
    if (_size + 1 > _capacity) { expand(); }    // 插入后会超出容量，需要扩容，此处省略扩容策略
    arrayCopy(_data+r+1, _data+r, _size-r, -1); // 从后向前，依次移动V[r:n]中的每个元素
    _data[r] = e;
    ++_size; // 更新向量的规模
}
```

> `arrayCopy`那行语句从功能上等价于`V[r+1:n+1] = V[r:n]`，就是将`V[r:n]`整体后移1个单元。
>
> 这里`arrayCopy`的最后一个参数表示从后向前更新（您可以自己想一下，如果从前向后更新会发生什么？）。`arrayCopy`的实现见配套代码。

不考虑扩容（前面已经证明，扩容的分摊复杂度是$O(1)$，它不会影响到其他操作的复杂度），则单次插入的时间复杂度为$\Theta(n-r)$，空间复杂度$O(1)$。

### 平均复杂度分析

为了更定量地分析插入操作的时间效率，引入一个新的复杂度分析策略：**平均复杂度**。

在介绍复杂度时曾经强调，复杂度是依赖于<u>数据规模</u>，不依赖于<u>具体情况</u>的分析手段。在**算法2.2A**中，数据规模通常认为是`n`，而`r`是具体情况带来的参数。为了研究不同具体情况对算法时间效率的影响，有三种常见的分析手段：

* **最坏时间复杂度**：研究在情况最坏的情况下的复杂度。

  很多算法有硬性的时间限制（如<u>在复试的机试中，通常要求输出结果的时间不能多于1s或2s</u>），此时常常使用最坏时间复杂度分析。这是最常用的时间复杂度分析。

* **最好时间复杂度**：研究在情况最好的情况下的复杂度。

  研究最好时间复杂度的意义远小于最坏时间复杂度。最好时间复杂度往往用于嘲讽某种算法的效率：在最好的情况下，这种算法的复杂度也只能达到XXXX，而我的新算法可以达到XXXX。

* **平均时间复杂度**：研究在平均情况下的复杂度。

  如果没有硬性的时间限制，则平均时间复杂度往往能更好地反映一个算法的总体时间效率。

  平均时间复杂度需要知道<u>各种情况的**先验**概率</u>，在这个概率的基础上计算$T(n)$的**数学期望**的复杂度。在针对显示数据的实验研究中，常见的假设包括正态分布、gamma分布和Poisson分布；而在《数据结构》学科中，通常假设成<u>等可能的分布</u>，以方便进行理论计算。

  > 分摊复杂度是一系列连续操作的平均效率，而平均复杂度是单次操作的期望效率。
  >
  > 分摊复杂度的一系列连续操作是有可能存在后效的，而平均复杂度只讨论单次操作的可能情况。
  >
  > 分摊复杂度需要指定每次进行何种的**基本操作**，而平均复杂度需要指定各种情况的**先验概率**。
  >
  > 这两个概念务必加以区分。

> 最坏、最好、平均时间复杂度对应统计里的**最大值**、**最小值**和**数学期望**。显然，其他统计量，比如**方差**、**标准差**，在分析的时候也是有价值的，也深得科研人员重视。但在《数据结构》的考试中，是不会涉及到这些统计量的分析的，只需要知道最坏、最好和平均时间复杂度的分析技术即可。

现在回到插入的**算法2.2A**，它的时间复杂度是$\Theta(n-r)$。显然，最好时间复杂度是$O(1)$（插入在末尾的情况），最坏时间复杂度是$\Theta(n)$（插入在开头的情况）。

为了求平均时间复杂度，一个合理的假设是，`r`的取值对于`[0:n]`之间的整数是等概率的。在这个假设下，容易算出单次插入的<u>平均时间复杂度</u>为$\Theta(n)$。

### 插入批量元素

如果要插入的不是单个元素，而是多个元素，情况会发生什么变化呢？

```c++
// 问题2.3 - 向量插入多个元素（向量合并）
// 给定：向量V[0:n]
// 输入：待插入的向量V1[0:n1]，目标的秩r
// 要求：将向量V1整体插入到V中，其中V1[0]插入到V[r]的位置上
```

在**问题2.3**中，需要插入`n1`个连续的元素。最简单的想法是直接调用`n1`次**算法2.2A**，然而这样的总时间复杂度高达平均$\Theta(n\cdot n_1)$，略显笨重。

您可以敏锐地发现，只要再次使用在讨论单元素插入时的分析方法，就可以得到更加高效的算法。

要将待插入的向量`V1`插入到`V[r]`，那么可以将原来的向量`V[0:n]`分成`V[0:r]`和`V[r:n]`两部分。

* 插入之前，向量是`V[0:r]`，`V[r:n]`。
* 插入之后，向量是`V[0:r]`，`V1`，`V[r:n]`。其中，`V[r:n]`被转移到了`V[r+n1:n+n1]`的位置上。

```c++
// 算法2.3A - 向量插入元素（批量）
template <typename T>
void Vector<T>::insert(const Vector<T>& V, Rank r) {
    int new_size = _size + V._size;                   // 计算插入之后的向量规模
    if (new_size > _capacity) { expand([=](int m) -> int {
        return max(new_size, default_expand_strategy(m));
    }); }        // 插入后会超出容量，需要扩容。这里省略指定的default_expand_strategy
    arrayCopy(_data+r+V._size, _data+r, _size-r, -1); // 从后向前，依次移动V[r:n]中的每个元素
    arrayCopy(_data+r, V._data, V._size);             // 依次插入V1中的元素
    _size = new_size; // 更新向量的规模
}
```

**算法2.3A**的平均时间复杂度为$\Theta(n+n_1)$，比连续调用`n1`次**算法2.2A**要优秀得多。**算法2.3A**中体现出的“<u>用块操作代替多次单元操作</u>”的思想，在以线性表为背景的算法设计题中应用广泛。

> **算法2.3A**仍然有继续改进的空间。
>
> 在**算法2.3A**的实现中，<u>插入操作和扩容操作</u>是解耦的。事实上，在扩容申请了新的数组空间之后，没有必要先把原数组的元素复制过去再移动。移动可以复制到新数组空间的同时进行，从而减少一次`arrayCopy`移动的时间。您可以自己实现这个改进版本的算法。

### 删除单个元素

删除元素是插入元素的逆操作。在插入元素时，让<u>被插入元素的后继后移</u>；因此在删除元素的时候，只需要让<u>被删除元素的后继前移</u>即可。

```c++
// 问题2.4 - 向量删除元素
// 给定：向量V[0:n]
// 输入：待删除元素的秩r
// 要求：将V[r]删除

// 算法2.4A - 向量删除元素（单元）
template<typename T>
void Vector<T>::remove(Rank r) {
    --_size;  // 更新向量的规模
    arrayCopy(_data+r, _data+r+1, _size-r); // 这里可以从前向后依次前移
    shrink(); // 如果有必要，则缩容
}
```

删除操作同样是时间复杂度$\Theta(n-r)$，平均时间复杂度$\Theta(n)$，空间复杂度$O(1)$。

对于批量删除操作（一次删除`V[r1:r2]`的所有元素），可以用与批量插入相似的方法解决，请您自己设计相关算法。

> 这里讨论的向量的“删除元素”严格地说是“循秩删除”。
>
> 另一种删除的方式是，“删除满足某一条件的所有元素”，这个问题将在后面的小节里讨论。

### 查找单个元素

在向量查找一个元素，只需要得到被查找元素的**秩**就可以了，因为`V[i]`的地址就是其所基于的数组的首地址偏移`i`个。和插入、删除相比，查找具有更加丰富的灵活性，甚至于一些编程语言（如SQL）的核心就是查找。

最简单的查找是**按值查找**。即，给定被查找元素的值，在数据结构中找到等于这个值的元素。

```c++
// 问题2.5 - 向量查找
// 给定：向量V[0:n]
// 输入：待查找的元素e
// 输出：元素e在向量V中的秩
```

对于这个问题，最简单的方案就是<u>检测向量中的每个元素</u>是否等于`e`，如果等于，就把它的秩返回。

```c++
// 算法2.5A
template<typename T>
Rank Vector<T>::find(T e) {
    for (Rank i = 0; i < _size; ++i) { // 检测每个元素是否等于e
        if (_data[i] == e) {
            return i;                 // 如果相等则返回秩
        }
    }
    return -1;                        // e不在向量中，返回-1
}
```

设`e`在向量中的秩为`r`，那么在<u>查找成功的情况</u>下，上述算法的时间复杂度为$\Theta(r)$。在<u>查找失败的情况</u>下，算法的时间复杂度为$\Theta(n)$。这里可以分析，在等可能条件下，查找成功时的平均时间复杂度是$\Theta(n)$。

> 查找成功的概率是一个很难假设的值，所以在分析平均时间复杂度时，通常只分析“查找成功时”和“查找失败时”的平均时间复杂度，而不会将它们混为一谈。

因为对于向量`V`和待查找元素`e`的情况没有更多的先验信息，所以暂时也没有比**算法2.5A**更高效的解决方案了。

> **利用信息思考**是计算机领域重要的思维方式。在设计算法时，应尽可能利用更多的先验信息。反之，如果先验信息不足，则算法的效率受到信息论限制，不可能会特别高。这个思维方式在后文介绍各种算法的设计过程时，还会反复出现。

但**算法2.5A**还是有一些值得推敲的地方：如果`e`在向量`V`中出现了多次，那么**算法2.5A**只会返回**最小的秩**。您可以思考一下，如何将其修改成返回**最大的秩**的算法？修改后的算法复杂度和**算法2.5A**有区别吗？

### 查找批量元素

进一步地，如何修改成返回**所有的秩**？因为要返回的是所有的秩，所以不能像**算法2.5A**或它的“最大秩修改版”那样，找到一个等于`e`的元素就直接`return`，必须老老实实地判断向量中的每个元素是否等于`e`。

将这个条件更加一般化，既然可以查找所有等于`e`的元素，那么对于任意给定的条件，也就可以查找所有满足该条件的元素。这个条件可以是“等于`e`”，也可以是“大于`e`”，甚至可能不但和元素本身有关，还和它的秩有关。

```c++
// 问题2.6 - 向量查找
// 给定：向量V[0:n]
// 输入：需要满足条件filter
// 输出：所有满足条件的元素在向量V中的秩
```

<u>以某种次序访问数据结构中的每个元素有且仅有一次</u>，这一过程称为**遍历**（traverse）。刚才讨论过，遍历的时候可能不只要用到元素本身，还需要用到它的秩，所以遍历函数`visit`不能只接受一个`T&`类型的元素本身的引用，还需要接受一个表示秩的参数。

```c++
// 算法2.6A
template<typename T>
void Vector<T>::traverse(function<void(Rank, T&)> visit) {
    for (Rank i = 0; i < _size; ++i) {
        visit(i, _data[i]);
    }
}
```

上面的代码是最为经典的向量遍历模式：从`V[0]`遍历到`V[n-1]`，称为**顺序遍历**。如果是反着从`V[n-1]`遍历到`V[0]`，则称为**倒序遍历**。在一些特殊的问题中，还可能会用到其他遍历方法。

> 在本节的后续小节里，会举例介绍这一点。

遍历要求访问数据结构的每个元素各一次。重复访问或者中途退出都不是完整的遍历。因此，如果不考虑访问的次序，且`visit`只会读写自己访问的那个元素，那么不同方式的遍历产生的结果是一样的。在批量查找这个场合就是这样，无论**算法2.6A**的遍历算法是怎么写的，都可以统一地通过下面的**算法2.6B**完成批量查找。

```c++
// 算法2.6B
template <typename T>
Vector<Rank> Vector<T>::findAll(function<bool(Rank, const T&)> filter) {
    Vector<Rank> temp;
    traverse([&temp](Rank index, const T& e) -> void {
        if (filter(index, e)) {
            temp.push_back(index);
        }
    });
    return temp;
}
```

> 其中`push_back`意为在尾部添加元素，即`insert(e, _size)`。

在`filter`的时间复杂度为$O(1)$的情况下，由于`push_back`的均摊复杂度为$O(1)$，所以遍历过程中，访问每个元素时需要的时间为$O(1)$；故上述算法的总体时间复杂度为$\Theta(n)$。

### 删除批量元素

这一小节讨论之前遗留下来的一个问题：如何删除向量中<u>所有满足条件</u>的元素？

> 这个问题和上一小节讨论的“如何*查找*向量中<u>所有满足条件</u>的元素”是非常相似的。

```c++
// 问题2.7 - 向量删除元素
// 给定：向量V[0:n]
// 输入：需要满足条件filter
// 要求：删除向量V中所有满足条件的元素
```

一个最朴素的思想是：在向量`V`中查找满足`filter`条件的元素，然后将它删除；直到没有满足`filter`条件的元素为止。

```c++
// 算法2.7A（逐个查找-逐个删除）
template <typename T>
void Vector<T>::removeAll(function<bool(Rank, const T&)> filter) {
    Rank index;
    while ((index = find(filter)) >= 0) {
        remove(index);
    }
}
```

这里仍然不妨假设`filter`的时间复杂度是$O(1)$，以排除`filter`对删除算法复杂度造成的影响。

上述**算法2.7A**的空间复杂度是$O(1)$，但是时间效率是很低的。设`r=index`，那么根据之前几个小节的实现，`find`的时间复杂度为$\Theta(r)$，而`remove`的时间复杂度为$\Theta(n-r)$，因此每一遍循环，时间复杂度均为$\Theta(n)$。在最坏的情况下，所有的`n`个元素都满足`filter`的条件，所以总的时间复杂度为$\Theta(n)+\Theta(n-1)+\dots+\Theta(1)=\Theta(n+(n-1)+\dots+1)=\Theta(n^2)$。

这个时间复杂度显然不能接受。之前介绍的“批量插入”和“批量查找”，都是线性时间复杂度，对于批量删除，无论如何都无法接受平方级的时间复杂度。

> 在设计算法的时候，题目不一定会给出要求的复杂度。这个时候，可以对比一下<u>“相似问题”的复杂度</u>。
>
> 为了降低时间复杂度，就要设法降低在算法中进行的**不必要工作**。在不必要工作中，最典型的一种是**重复工作**，它代表了在算法中，反复计算了同一算式造成的时间效率浪费。

对于**算法2.7A**而言，它有一项非常明显的重复工作：假设第一遍`find`的返回值是`r`并删掉了`V[r]`，那么在进行第二遍`find`的时候，`V[0:r]`中的元素仍然被检索了一遍，但实际上，第一遍`find`已经检索过它们了。

为了消去这一重复工作，则在第二遍`find`的时候，不应该再从`V[0]`开始检索，而应该从`V[r]`开始检索。

```c++
// 算法2.7B（一次查找-逐个删除）
template <typename T>
void Vector<T>::removeAll(function<bool(Rank, const T&)> filter) {
    for (Rank i = 0; i < _size; ) {
        if (filter(i, _data[i])) {
            remove(i);
        } else {
            ++i; // 注意，如果remove了则不需要++i，否则会跳过1个元素
        }
    }
}
```

> 和这种方法相似的一个方法，是用`findAll`找出所有满足`filter`的元素，然后逐个删除。您可以自己实现它：它会多耗费最坏$\Theta(n)$的空间，而对时间效率没有影响。

然而，在最坏的情况下（所有元素都要被删除），光是`remove`就要花费$\Theta(n^2)$的时间，**算法2.7B**的优化程度仍然不够。

因此，下一步优化就要从`remove`入手，需要将`remove`的工作展开来，看看其中哪些是不必要的。在`remove`中，主要消耗时间的是<u>元素移动</u>的操作。您可以发现，如果`V[0:i]`中有`k`个元素要删除，那么最后一个元素`V[i-1]`就要向前移动`k`次：依次移动到`V[i-2]`、`V[i-3]`、……、`V[i-k-1]`的位置上。看上去这些操作并没有**重复工作**，但它们是另一类典型的不必要工作：**不到位工作**。这一系列的移动被拆成了`k`次，而实际上是可以一步到位，直接从`V[i-1]`移动到目标位置`V[i-k-1]`的。

为什么可以直接移动到目标位置呢？注意到，在**算法2.7B**中，当检索到`V[i]`的时候，`V[0:i]`的所有元素都已经被检索过了，因此`k`的值已经确定了，并且前`i-1`个元素已经移动到了正确的目标位置。所以您可以用归纳法的思路，证明直接移动的正确性。证明完成之后，剩下的就只有编码的工作了。

```c++
// 算法2.7C（一次查找-一次删除）
template <typename T>
void Vector<T>::removeAll(function<bool(Rank, const T&)> filter) {
    int k = 0;      // 用来记录偏移量，即V[0:i]中满足filter的数量
    for (Rank i = 0; i < _size; ++i) {
        if (filter(i, _data[i])) {
            ++k;    // 满足filter条件，记录偏移量
        } else {
            _data[i-k] = _data[i]; // 不满足filter，移动元素
        }
    }
    _size -= k;     // 直接缩减_size抛弃掉末尾的元素
    shrink();       // 如果有必要，则缩容
}
```

非常显然，现在时间复杂度被缩减到$\Theta(n)$了。

> 可以看出，**算法2.7C**中还是有一些不必要工作。在`k=0`的时候，会产生没有意义的赋值操作。但这一数量的不必要操作，不会对算法的时间复杂度产生影响，所以通常优化到这个层次就可以了。您可以自己尝试将没有意义的赋值操作去掉。

**算法2.7C**的思路可以被概括为**快慢指针**。快指针即探查指针，指向`V[i]`；慢指针即更新指针，指向`V[i-k]`。快指针找到需要保留的元素，然后将它们移动到慢指针的位置处。

### 随机置乱

一般数据结构重点讨论的只有**插入**、**删除**和**查找**三种基本操作，但向量作为一种非常基础的数据结构，经常被用来在考试中作为算法设计题的背景。下面这两个小节分别从熵增和熵减的角度出发，讨论**置乱**和**排序**的算法。

这一小节先讨论置乱。



### 归并排序

### 基于比较的排序

### 折半查找

### 简单尾递归

### 无序向量的唯一化

### 有序向量的唯一化

