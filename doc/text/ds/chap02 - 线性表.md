# 线性表

**线性表**是指相同类型的有限个数据组成的序列。在这本笔记中采用的是清华大学教材的分法，将线性表包括**向量**（vector）和**列表**（list）两种形式，分别对应C++ STL里的`vector`和`list`。

> 在另一些教材中，这两个词被称为**顺序表**和**链表**，分别对应Java里的`ArrayList`和`LinkedList`。向量（顺序表）和列表（链表）这两对概念通常可以混用。

向量和列表代表着两种最基本的数据结构组织形式：**顺序结构**和**链式结构**。本章在分别介绍这两种数据结构之后，将会进一步介绍这两种结构级联得到的**分块结构**。

## 向量

> 中国革命必须分为两个步骤。第一步，改变这个殖民地、半殖民地、半封建的社会形态，使之变成一个独立的民主主义的社会。第二步，使革命向前发展，建立一个社会主义的社会。
>
> ——Chairman Mao

**向量**（vector）是一个基于**数组**（array）的数据结构，因此向量在内存中占据的是一段连续的空间。

### 向量和数组

**元素**（element）代表数据结构中的单个数据单元，它可能是基本数据类型、结构体、函数等。一个数据结构中的元素具有统一的类型。所有数据结构都是由元素构成的。

**线性表**是元素成线性排列的表。作为一种基于数组的线性表，<u>向量的元素次序和数组的元素次序相同</u>。如果一个向量`V`基于数组`A[0:m]`构建，那么向量`V`的第`i`个元素就是`A[i]`。

> 说向量元素的**物理次序**和**逻辑次序**相同是不妥的，尽管这种说法可能能帮助您理解向量，但可能会引起新的混乱。
>
> 在《计算机组成原理》和《操作系统》学科中您将会看到，<u>“物理”和“逻辑”这两个相对的名词一般用于**实际内存**和**虚拟内存**的场合</u>。数组在虚拟内存上的逻辑地址总是连续的，但在实际内存上的物理地址受操作系统调度影响，可能由多个不连续的部分组成。所以用“物理次序”这样一个有歧义的名词是不妥的。

需要注意的是，尽管向量和它基于的数组在<u>元素次序上相同</u>，但在<u>元素数量上是不一定相同</u>的。数组的元素数量总是恒定的，而向量的元素数量是<u>运行时可变的</u>。

记一个长度为`n`的向量为`V[0:n] = { V[0], V[1], ..., V[n-1] }`，则`n`称为向量`V`的**规模**（size），而称为向量`V`分配的内存空间可以容纳的元素数量`m`（即它所基于的数组的规模）为向量`V`的**容量**（capacity）。

> 一个向量的<u>规模必定不大于容量</u>。在不超过容量的前提下，向量的规模可以灵活变化，从而赋予了它比数组更高的灵活性。

对于向量中的每一个元素`V[i]`来说，它前面的元素称为它的**前驱**（predecessor），它后面的元素称为它的**后继**（successor），特别地，和它位置相邻的前驱，也就是**直接前驱**为`V[i-1]`，相应地，**直接后继**为`V[i+1]`。所有的前驱构成了**前缀**（prefix），也就是`V[0:i]`；所有的后继构成了**后缀**（suffix），也就是`V[i+1:n]`。

### 循秩访问

在本笔记中介绍的各种数据结构，将从一个非常小的基础模板开始逐步添加功能。您可以跟随笔记的正文自己实现模板，或参考配套代码中的模板。

> 为了让模板更有用，配套代码里还实现了一些和《数据结构》知识关系不大的函数。这些内容在笔记正文中不会介绍。

在上一小节中已经介绍过向量的基本元素：用于存放数据的数组，以及规模、容量两个基本属性。据此，可以写出一个基本的框架如下。

```c++
template <typename T>
class Vector {
    T* _data;      // 向量所基于的数组
    int _capacity; // 向量的容量
    int _size;     // 向量的规模
public:
    int capacity() const { return _capacity; }
    int size() const { return _size; }
    T* data() const { return _data; }
};
```

> 这里将规模和容量都定义成了**私有**（private）变量，并提供了公共的`getter`方法。因为向量外的代码不应该直接修改它们。
>
> 之后介绍的数据结构，为简单起见不再列出公共的`getter`方法。

对于向量中的元素，其访问方式称为**循秩访问**。称元素`V[i]`在向量`V`中的序号，也就是`i`，为它的**秩**（rank）。对于建立在数组`A`上的向量`V`，因为`V`和`A`的元素次序是一致的，所以`V[i] = A[i] = *(A+i)`。因此，只要知道一个元素的秩，就可以在$O(1)$的时间内访问该元素。

```c++
typedef int Rank;

template <typename T>
T& Vector<T>::operator[](Rank index) const {
    return _data[index];
}
```

> 如果`index`超出了`capacity`，会发生数组越界，可能引起**段错误**（segmentation fault）。如果追求稳健性，应当增加一个`index < capacity`的判断。但为效率起见，这里舍弃了这一判断。
>
> 后文中同样会舍弃一些稳健性的考量。

### 装填因子

设向量的容量为`m`，规模为`n`，则称比值`n/m`为**装填因子**（load factor）。正常情况下，这是一个`[0,1]`之间的数。装填因子是衡量向量效率的重要指标。

<u>如果装填因子过小</u>，则会造成内存浪费：申请了巨大的数组，但其中只有少量的单元被向量中的元素用到，其他单元都被闲置了。

<u>如果装填因子过大</u>（超过1），则会引发数组越界，造成段错误。

> 刚开始的时候，装填因子一定是在`[0,1]`之间的。
>
> 但因为数组的容量`m`是固定的，而向量的规模`n`是动态的，所以一开始分配的`m`可能后来会不够用，从而产生装填因子大于1的问题。

为了让装填因子保持在一个合理的范围，需要允许动态地改变容量`m`的值。令m增大的操作称为**扩容**，令m减小的操作称为**缩容**。

### 改变向量的容量

首先来看向量扩容。

```c++
// 问题2.1 - 向量扩容
// 分析合适的方式让向量扩容
```

这就不是上一章那种简单的算法问题了，需要逐步分析。

首先，直接在原向量所占空间后面增加“一条尾巴”是不现实的，因为原向量所占空间后面的地址，可能已经被分配给了其他变量。因此，合适的做法是<u>重新建立一个更大的数组，将原向量的数据复制到新数组中，再把原数组的空间释放掉</u>。

> 注意这个操作对于缩容也是一样的，只是缩容是建立了一个更小的数组。

```c++
// 算法2.1A
template <typename T>
void Vector<T>::set_capacity(int new_capacity) {
    T* old = _data;
    _data = new T[_capacity = new_capacity];
    arrayCopy(_data, old, _size);
    delete[] old;
}
```

> 这里`arrayCopy(T* dst, T* src, int size)`函数用来将大小为`size`的数组从`src`移动到`dst`位置。

设`new_capacity`为`M`，则上述**算法2.1A**的时间和空间复杂度均为$O(M)$。

### 等差扩容和等比扩容

**算法2.1A**仍然没有解决关键问题：在实际使用中，用户不一定知道应该扩容到多少：如果新的容量`M`比较小，就可能再次发生数组越界，就需要再次调用`set_capacity`函数扩容，<u>消耗时间</u>；如果`M`比较大，则装填因子太低，<u>浪费空间</u>。

<u>所以，向量的设计者应该提供一个合理的扩容规则</u>。如果用户知道怎么扩容当然最好，但当用户不知道应该扩容到多少的时候，这个<u>合理的扩容规则</u>给了他们一个备选项。只要选择按照向量设计者提供的规则来扩容，使得时间和空间效率都不会太低。

那么应该如何设计扩容规则呢？

第一种方法是<u>按照等差数列扩容</u>，规定一个公差`d`，每次扩容都让容量`+d`（**固定扩容**），也就是从`m`变成`m+d`；第二种方法是<u>按照等比数列扩容</u>，规定一个公比`q`，每次扩容都让容量`*q`（**比例扩容**），也就是从`m`变成`qm`。

```c++
// 算法2.1B - 基本框架
template <typename T>
void Vector<T>::expand(function<int(int)> strategy) {
    set_capacity(strategy(_capacity));
}

// 算法2.1B1 - 固定扩容
template<typename T>
void expandByAP(Vector<T>& V, int d) {
    V.expand([=](int m) -> int { return m + d; });
}

// 算法2.1B2 - 比例扩容
template<typename T>
void expandByGP(Vector<T>& V, double q) {
    V.expand([=](int m) -> int { return (int)(m * q); });
}
```

> 这里允许**比例因子**`q`不是整数。实际使用时，如果比例因子是整数，则应当声明`q`为`int`类型以加快运算速度。特别地，如果`q`是2的幂次，可以写成移位计算的形式进一步加快运算速度。

很显然，在`strategy`函数只进行了简单的加（或乘）运算的情况下，`expand`只进行了一次`set_capacity`的操作，其效率和单次的`set_capacity`基本一致。

那么，<u>固定扩容和比例扩容</u>这两种扩容方式哪种更好？它们都只调用了一次`expand`，所以它们的效率是一样的吗？

显然不是的。设计**算法2.1B1**和**算法2.1B2**的原理，是按照等差或等比“<u>*数列*</u>”扩容，而不是“<u>*一次*</u>”扩容。所以评价这两种扩容规则的标准，不是<u>进行一次扩容</u>的效率或<u>进行一次扩容</u>后的装填因子，而是比较<u>一系列扩容</u>操作的总体效率和在这<u>一系列扩容</u>操作中的平均装填因子。用已有的复杂度分析工具不足以对这两种策略的效率进行准确评价。

### 分摊复杂度分析

为了对**一系列操作**进行分析，需要引入新的复杂度分析标准。

一般地，假设$O_1, O_2, \dots, O_n$是连续进行的`n`次操作，则当$n\to\infty$，这`n`次<u>连续操作所用时间的平均值的复杂度</u>，称为这一操作的**分摊复杂度**，对分摊复杂度的分析称为**分摊分析**。分摊分析的原则之一是：<u>使用相同效果的操作序列</u>。所以，要比较**算法2.1B1**和**算法2.1B2**，不应该把每次操作取为"进行一次扩容"（因为两种方法扩容量不一样)，而应该取为“向量`V`的规模增加`1`”。连续进行`n`次操作，就可以考虑向量`V`的规模从`0`增长为`n`的过程。

**算法2.1B1**中，容量依次被扩充为$d,2d,3d,\dots,n$，共进行$n/d$次扩容。

因此，分摊复杂度为：
$$
T(n) = \frac{d + 2d + 3d + \dots + n}n = \frac{(n/d)\cdot d + (n/d)(n/d-1)/2\cdot d}n = \frac{n/d+1}2 = O\left(\frac nd\right)
$$
另一方面，进行k次扩容之后的装填因子至少为$\frac{kd}{(k+1)d}=\frac{k}{k+1}$，当$k\to\infty$时，装填因子趋于**100%**。

**算法2.1B2**中，容量被依次扩充为$q,q^2,q^3,\dots,n$，共进行$\log _q n$次扩容。

因此，分摊复杂度为：
$$
T(n)=\frac{q+q^2+q^3+\dots+n}n=\frac{q\cdot\frac{1-n}{1-q}}n=O(1)
$$
另一方面，装填因子不断在$[\frac 1q,1]$之间线性增长，平均装填因子为$\frac{1+q}{2q}$。可以看出，不管怎样选择`q`，对分摊复杂度都没有影响，而更小的`q`能够带来更大的平均装填因子。因此，在实际的向量扩容过程中，总是选择`q = 2`，这时平均装填因子为**75%**。

从上面的分摊分析中可以看出，**算法2.1B1**尽管能够带来更高的装填因子，但时间效率略有不足；而**算法2.1B2**则在保证装填因子不太低的情况下，时间效率非常优秀。

所以，通常向量扩容会选择**算法2.1B2**的规则。

> 不过，**算法2.1B2**也有其劣势，就是容量越大，装填因子不高带来的空间浪费愈发明显，所以有些对空间要求较高的情况下，也采用二者相结合的方式：<u>在容量比较小时加倍扩容、在容量比较大的时候固定扩容</u>，在《计算机网络原理》的学习中您将看到二者相结合的例子。

### 缩容

介绍完扩容，缩容的方法也呼之欲出了：无非是**固定缩容**或者**减半缩容**。您可以自己完成这两个算法。

缩容的重要性远不如扩容，因为缩容降低`m`之后，如果`n`又扩大回去了，就又要扩容回去，这一缩一扩浪费了不少时间，而发挥的价值甚微（除非在这段缩、扩之间的时间里，释放出的内存另有他用）。为了避免一缩一扩浪费时间，通常会规定一个**缩容阈值**，当装填因子小于这个阈值的时候才会缩容。

对于常用的<u>比例扩容+比例缩容</u>的策略，您可以自己证明，只要<u>缩容阈值<50%</u>，就可以保证对于任意的操作序列（每个操作是`n`加一或减一），扩容和缩容的<u>综合分摊复杂度</u>为$O(1)$。在实际的向量中，为了方便计算，缩容阈值通常使用25%、12.5%等数值，取为0（**禁止缩容**）也是一个常见的策略。

### 插入单个元素

对于任何数据结构，都有三种基本的操作：

* **插入**（insert）：向数据结构中插入一个元素。

* **查找**（find）：查找一个元素在数据结构中的位置。

* **删除**（delete）：从数据结构中移除一个元素。

  > 因为`delete`是许多语言的关键字，所以在编程的时候常用`remove`代替它。

下面就以向量为例，分别介绍这三种基本的操作。

```c++
// 问题2.2 - 向量插入元素
// 给定：向量V[0:n]
// 输入：待插入的元素e，目标的秩r
// 要求：将元素e插入到V[r]的位置上
```

首先讨论插入。

要将待插入的元素`e`插入到`V[r]`，那么可以将原来的向量`V[0:n]`分成`V[0:r]`和`V[r:n]`两部分。

* 插入之前，向量是`V[0:r]`，`V[r:n]`。
* 插入之后，向量是`V[0:r]`，`e`，`V[r:n]`。

可以发现，在插入的前后，前一段`V[0:r]`的位置是不变的，而后一段`V[r:n]`需要整体向后移动1个单元的位置。据此，可以设计下面的算法。

```c++
// 算法2.2A
template <typename T>
void Vector<T>::insert(T e, Rank r) {
    if (_size + 1 > _capacity) { expand(); }    // 插入后会超出容量，需要扩容，此处省略扩容策略
    arrayCopy(_data+r+1, _data+r, _size-r, -1); // 从后向前，依次移动V[r:n]中的每个元素
    _data[r] = e;
    ++_size; // 更新向量的规模
}
```

> `arrayCopy`那行语句从功能上等价于`V[r+1:n+1] = V[r:n]`，就是将`V[r:n]`整体后移1个单元。
>
> 这里`arrayCopy`的最后一个参数表示从后向前更新（您可以自己想一下，如果从前向后更新会发生什么？）。`arrayCopy`的实现见配套代码。

不考虑扩容（前面已经证明，扩容的分摊复杂度是$O(1)$，它不会影响到其他操作的复杂度），则单次插入的时间复杂度为$\Theta(n-r)$，空间复杂度$O(1)$。

### 平均复杂度分析

为了更定量地分析插入操作的时间效率，引入一个新的复杂度分析策略：**平均复杂度**。

在介绍复杂度时曾经强调，复杂度是依赖于<u>数据规模</u>，不依赖于<u>具体情况</u>的分析手段。在**算法2.2A**中，数据规模通常认为是`n`，而`r`是具体情况带来的参数。为了研究不同具体情况对算法时间效率的影响，有三种常见的分析手段：

* **最坏时间复杂度**：研究在情况最坏的情况下的复杂度。

  很多算法有硬性的时间限制（如<u>在复试的机试中，通常要求输出结果的时间不能多于1s或2s</u>），此时常常使用最坏时间复杂度分析。这是最常用的时间复杂度分析。

* **最好时间复杂度**：研究在情况最好的情况下的复杂度。

  研究最好时间复杂度的意义远小于最坏时间复杂度。最好时间复杂度往往用于嘲讽某种算法的效率：在最好的情况下，这种算法的复杂度也只能达到XXXX，而我的新算法可以达到XXXX。

* **平均时间复杂度**：研究在平均情况下的复杂度。

  如果没有硬性的时间限制，则平均时间复杂度往往能更好地反映一个算法的总体时间效率。

  平均时间复杂度需要知道<u>各种情况的**先验**概率</u>，在这个概率的基础上计算$T(n)$的**数学期望**的复杂度。在针对显示数据的实验研究中，常见的假设包括正态分布、gamma分布和Poisson分布；而在《数据结构》学科中，通常假设成<u>等可能的分布</u>，以方便进行理论计算。

  > 分摊复杂度是一系列连续操作的平均效率，而平均复杂度是单次操作的期望效率。
  >
  > 分摊复杂度的一系列连续操作是有可能存在后效的，而平均复杂度只讨论单次操作的可能情况。
  >
  > 分摊复杂度需要指定每次进行何种的**基本操作**，而平均复杂度需要指定各种情况的**先验概率**。
  >
  > 这两个概念务必加以区分。

> 最坏、最好、平均时间复杂度对应统计里的**最大值**、**最小值**和**数学期望**。显然，其他统计量，比如**方差**、**标准差**，在分析的时候也是有价值的，也深得科研人员重视。但在《数据结构》的考试中，是不会涉及到这些统计量的分析的，只需要知道最坏、最好和平均时间复杂度的分析技术即可。

现在回到插入的**算法2.2A**，它的时间复杂度是$\Theta(n-r)$。显然，最好时间复杂度是$O(1)$（插入在末尾的情况），最坏时间复杂度是$\Theta(n)$（插入在开头的情况）。

为了求平均时间复杂度，一个合理的假设是，`r`的取值对于`[0:n]`之间的整数是等概率的。在这个假设下，容易算出单次插入的<u>平均时间复杂度</u>为$\Theta(n)$。

### 插入批量元素

如果要插入的不是单个元素，而是多个元素，情况会发生什么变化呢？

```c++
// 问题2.3 - 向量插入多个元素（向量合并）
// 给定：向量V[0:n]
// 输入：待插入的向量V1[0:n1]，目标的秩r
// 要求：将向量V1整体插入到V中，其中V1[0]插入到V[r]的位置上
```

在**问题2.3**中，需要插入`n1`个连续的元素。最简单的想法是直接调用`n1`次**算法2.2A**，然而这样的总时间复杂度高达平均$\Theta(n\cdot n_1)$，略显笨重。

您可以敏锐地发现，只要再次使用在讨论单元素插入时的分析方法，就可以得到更加高效的算法。

要将待插入的向量`V1`插入到`V[r]`，那么可以将原来的向量`V[0:n]`分成`V[0:r]`和`V[r:n]`两部分。

* 插入之前，向量是`V[0:r]`，`V[r:n]`。
* 插入之后，向量是`V[0:r]`，`V1`，`V[r:n]`。其中，`V[r:n]`被转移到了`V[r+n1:n+n1]`的位置上。

```c++
// 算法2.3A - 向量插入元素（批量）
template <typename T>
void Vector<T>::insert(const Vector<T>& V, Rank r) {
    int new_size = _size + V._size;                   // 计算插入之后的向量规模
    if (new_size > _capacity) { expand([=](int m) -> int {
        return max(new_size, default_expand_strategy(m));
    }); }        // 插入后会超出容量，需要扩容。这里省略指定的default_expand_strategy
    arrayCopy(_data+r+V._size, _data+r, _size-r, -1); // 从后向前，依次移动V[r:n]中的每个元素
    arrayCopy(_data+r, V._data, V._size);             // 依次插入V1中的元素
    _size = new_size; // 更新向量的规模
}
```

**算法2.3A**的平均时间复杂度为$\Theta(n+n_1)$，比连续调用`n1`次**算法2.2A**要优秀得多。**算法2.3A**中体现出的“<u>用块操作代替多次单元操作</u>”的思想，在以线性表为背景的算法设计题中应用广泛。

> **算法2.3A**仍然有继续改进的空间。
>
> 在**算法2.3A**的实现中，<u>插入操作和扩容操作</u>是解耦的。事实上，在扩容申请了新的数组空间之后，没有必要先把原数组的元素复制过去再移动。移动可以复制到新数组空间的同时进行，从而减少一次`arrayCopy`移动的时间。您可以自己实现这个改进版本的算法。

### 删除单个元素

删除元素是插入元素的逆操作。在插入元素时，让<u>被插入元素的后继后移</u>；因此在删除元素的时候，只需要让<u>被删除元素的后继前移</u>即可。

```c++
// 问题2.4 - 向量删除元素
// 给定：向量V[0:n]
// 输入：待删除元素的秩r
// 要求：将V[r]删除

// 算法2.4A - 向量删除元素（单元）
template<typename T>
void Vector<T>::remove(Rank r) {
    --_size;  // 更新向量的规模
    arrayCopy(_data+r, _data+r+1, _size-r); // 这里可以从前向后依次前移
    shrink(); // 如果有必要，则缩容
}
```

删除操作同样是时间复杂度$\Theta(n-r)$，平均时间复杂度$\Theta(n)$，空间复杂度$O(1)$。

对于批量删除操作（一次删除`V[r1:r2]`的所有元素），可以用与批量插入相似的方法解决，请您自己设计相关算法。

> 这里讨论的向量的“删除元素”严格地说是“循秩删除”。
>
> 另一种删除的方式是，“删除满足某一条件的所有元素”，这个问题将在后面的小节里讨论。

### 查找单个元素

在向量查找一个元素，只需要得到被查找元素的**秩**就可以了，因为`V[i]`的地址就是其所基于的数组的首地址偏移`i`个。和插入、删除相比，查找具有更加丰富的灵活性，甚至于一些编程语言（如SQL）的核心就是查找。

最简单的查找是**按值查找**。即，给定被查找元素的值，在数据结构中找到等于这个值的元素。

```c++
// 问题2.5 - 向量查找
// 给定：向量V[0:n]
// 输入：待查找的元素e
// 输出：元素e在向量V中的秩
```

对于这个问题，最简单的方案就是<u>检测向量中的每个元素</u>是否等于`e`，如果等于，就把它的秩返回。

```c++
// 算法2.5A
template<typename T>
Rank Vector<T>::find(T e) {
    for (Rank i = 0; i < _size; ++i) { // 检测每个元素是否等于e
        if (_data[i] == e) {
            return i;                 // 如果相等则返回秩
        }
    }
    return -1;                        // e不在向量中，返回-1
}
```

设`e`在向量中的秩为`r`，那么在<u>查找成功的情况</u>下，上述算法的时间复杂度为$\Theta(r)$。在<u>查找失败的情况</u>下，算法的时间复杂度为$\Theta(n)$。这里可以分析，在等可能条件下，查找成功时的平均时间复杂度是$\Theta(n)$。

> 查找成功的概率是一个很难假设的值，所以在分析平均时间复杂度时，通常只分析“查找成功时”和“查找失败时”的平均时间复杂度，而不会将它们混为一谈。

因为对于向量`V`和待查找元素`e`的情况没有更多的先验信息，所以暂时也没有比**算法2.5A**更高效的解决方案了。

> **利用信息思考**是计算机领域重要的思维方式。在设计算法时，应尽可能利用更多的先验信息。反之，如果先验信息不足，则算法的效率受到信息论限制，不可能会特别高。这个思维方式在后文介绍各种算法的设计过程时，还会反复出现。

但**算法2.5A**还是有一些值得推敲的地方：如果`e`在向量`V`中出现了多次，那么**算法2.5A**只会返回**最小的秩**。您可以思考一下，如何将其修改成返回**最大的秩**的算法？修改后的算法复杂度和**算法2.5A**有区别吗？

### 查找批量元素

进一步地，如何修改成返回**所有的秩**？因为要返回的是所有的秩，所以不能像**算法2.5A**或它的“最大秩修改版”那样，找到一个等于`e`的元素就直接`return`，必须老老实实地判断向量中的每个元素是否等于`e`。

将这个条件更加一般化，既然可以查找所有等于`e`的元素，那么对于任意给定的条件，也就可以查找所有满足该条件的元素。这个条件可以是“等于`e`”，也可以是“大于`e`”，甚至可能不但和元素本身有关，还和它的秩有关。

```c++
// 问题2.6 - 向量查找
// 给定：向量V[0:n]
// 输入：需要满足条件filter
// 输出：所有满足条件的元素在向量V中的秩
```

<u>以某种次序访问数据结构中的每个元素有且仅有一次</u>，这一过程称为**遍历**（traverse）。刚才讨论过，遍历的时候可能不只要用到元素本身，还需要用到它的秩，所以遍历函数`visit`不能只接受一个`T&`类型的元素本身的引用，还需要接受一个表示秩的参数。

```c++
// 算法2.6A
template<typename T>
void Vector<T>::traverse(function<void(Rank, const T&)> visit) const {
    for (Rank i = 0; i < _size; ++i) {
        visit(i, _data[i]);
    }
}
```

上面的代码是最为经典的向量遍历模式：从`V[0]`遍历到`V[n-1]`，称为**顺序遍历**。如果是反着从`V[n-1]`遍历到`V[0]`，则称为**倒序遍历**。在一些特殊的问题中，还可能会用到其他遍历方法。

> 在本节的后续小节里，会举例介绍这一点。

遍历要求访问数据结构的每个元素各一次。重复访问或者中途退出都不是完整的遍历。因此，如果不考虑访问的次序，且`visit`只会读写自己访问的那个元素，那么不同方式的遍历产生的结果是一样的。在批量查找这个场合就是这样，无论**算法2.6A**的遍历算法是怎么写的，都可以统一地通过下面的**算法2.6B**完成批量查找。

```c++
// 算法2.6B
template <typename T>
Vector<Rank> Vector<T>::findAll(function<bool(Rank, const T&)> filter) const {
    Vector<Rank> temp;
    traverse([=, &temp](Rank index, const T& e) -> void {
        if (filter(index, e)) {
            temp.push_back(index);
        }
    });
    return temp;
}
```

> 其中`push_back`意为在尾部添加元素，即`insert(e, _size)`。

在`filter`的时间复杂度为$O(1)$的情况下，由于`push_back`的均摊复杂度为$O(1)$，所以遍历过程中，访问每个元素时需要的时间为$O(1)$；故上述算法的总体时间复杂度为$\Theta(n)$。

### 删除批量元素

这一小节讨论之前遗留下来的一个问题：如何删除向量中<u>所有满足条件</u>的元素？

> 这个问题和上一小节讨论的“如何*查找*向量中<u>所有满足条件</u>的元素”是非常相似的。

```c++
// 问题2.7 - 向量删除元素
// 给定：向量V[0:n]
// 输入：需要满足条件filter
// 要求：删除向量V中所有满足条件的元素
```

一个最朴素的思想是：在向量`V`中查找满足`filter`条件的元素，然后将它删除；直到没有满足`filter`条件的元素为止。

```c++
// 算法2.7A（逐个查找-逐个删除）
template <typename T>
void Vector<T>::removeAll(function<bool(Rank, const T&)> filter) {
    Rank index;
    while ((index = find(filter)) >= 0) {
        remove(index);
    }
}
```

这里仍然不妨假设`filter`的时间复杂度是$O(1)$，以排除`filter`对删除算法复杂度造成的影响。

上述**算法2.7A**的空间复杂度是$O(1)$，但是时间效率是很低的。设`r=index`，那么根据之前几个小节的实现，`find`的时间复杂度为$\Theta(r)$，而`remove`的时间复杂度为$\Theta(n-r)$，因此每一遍循环，时间复杂度均为$\Theta(n)$。在最坏的情况下，所有的`n`个元素都满足`filter`的条件，所以总的时间复杂度为$\Theta(n)+\Theta(n-1)+\dots+\Theta(1)=\Theta(n+(n-1)+\dots+1)=\Theta(n^2)$。

这个时间复杂度显然不能接受。之前介绍的“批量插入”和“批量查找”，都是线性时间复杂度，对于批量删除，无论如何都无法接受平方级的时间复杂度。

> 在设计算法的时候，题目不一定会给出要求的复杂度。这个时候，可以对比一下<u>“相似问题”的复杂度</u>。
>
> 为了降低时间复杂度，就要设法降低在算法中进行的**不必要工作**。在不必要工作中，最典型的一种是**重复工作**，它代表了在算法中，反复计算了同一算式造成的时间效率浪费。

对于**算法2.7A**而言，它有一项非常明显的重复工作：假设第一遍`find`的返回值是`r`并删掉了`V[r]`，那么在进行第二遍`find`的时候，`V[0:r]`中的元素仍然被检索了一遍，但实际上，第一遍`find`已经检索过它们了。

为了消去这一重复工作，则在第二遍`find`的时候，不应该再从`V[0]`开始检索，而应该从`V[r]`开始检索。

```c++
// 算法2.7B（一次查找-逐个删除）
template <typename T>
void Vector<T>::removeAll(function<bool(Rank, const T&)> filter) {
    for (Rank i = 0; i < _size; ) {
        if (filter(i, _data[i])) {
            remove(i);
        } else {
            ++i; // 注意，如果remove了则不需要++i，否则会跳过1个元素
        }
    }
}
```

> 和这种方法相似的一个方法，是用`findAll`找出所有满足`filter`的元素，然后逐个删除。您可以自己实现它：它会多耗费最坏$\Theta(n)$的空间，而对时间效率没有影响。

然而，在最坏的情况下（所有元素都要被删除），光是`remove`就要花费$\Theta(n^2)$的时间，**算法2.7B**的优化程度仍然不够。

因此，下一步优化就要从`remove`入手，需要将`remove`的工作展开来，看看其中哪些是不必要的。在`remove`中，主要消耗时间的是<u>元素移动</u>的操作。您可以发现，如果`V[0:i]`中有`k`个元素要删除，那么最后一个元素`V[i-1]`就要向前移动`k`次：依次移动到`V[i-2]`、`V[i-3]`、……、`V[i-k-1]`的位置上。看上去这些操作并没有**重复工作**，但它们是另一类典型的不必要工作：**不到位工作**。这一系列的移动被拆成了`k`次，而实际上是可以一步到位，直接从`V[i-1]`移动到目标位置`V[i-k-1]`的。

为什么可以直接移动到目标位置呢？注意到，在**算法2.7B**中，当检索到`V[i]`的时候，`V[0:i]`的所有元素都已经被检索过了，因此`k`的值已经确定了，并且前`i-1`个元素已经移动到了正确的目标位置。所以您可以用归纳法的思路，证明直接移动的正确性。证明完成之后，剩下的就只有编码的工作了。

```c++
// 算法2.7C（一次查找-一次删除）
template <typename T>
void Vector<T>::removeAll(function<bool(Rank, const T&)> filter) {
    int k = 0;      // 用来记录偏移量，即V[0:i]中满足filter的数量
    for (Rank i = 0; i < _size; ++i) {
        if (filter(i, _data[i])) {
            ++k;    // 满足filter条件，记录偏移量
        } else {
            _data[i-k] = _data[i]; // 不满足filter，移动元素
        }
    }
    _size -= k;     // 直接缩减_size抛弃掉末尾的元素
    shrink();       // 如果有必要，则缩容
}
```

非常显然，现在时间复杂度被缩减到$\Theta(n)$了。

> 可以看出，**算法2.7C**中还是有一些不必要工作。在`k=0`的时候，会产生没有意义的赋值操作。但这一数量的不必要操作，不会对算法的时间复杂度产生影响，所以通常优化到这个层次就可以了。您可以自己尝试将没有意义的赋值操作去掉。

**算法2.7C**的思路可以被概括为**快慢指针**。快指针即探查指针，指向`V[i]`；慢指针即更新指针，指向`V[i-k]`。快指针找到需要保留的元素，然后将它们移动到慢指针的位置处。

### 随机置乱

一般数据结构重点讨论的只有**插入**、**删除**和**查找**三种基本操作，但向量作为一种非常基础的数据结构，经常被用来在考试中作为算法设计题的背景。下面这两个小节分别从熵增和熵减的角度出发，讨论**置乱**和**排序**的算法。

这一小节先讨论置乱。

```c++
// 问题2.8 - 向量置乱
// 给定：向量V[0:n]
//      在这个问题中，假定rand()能随机产生一个正整数
// 要求：随机打乱向量V中的元素
```

> 现实中的`rand`是**伪随机**。对于同一个种子，生成的伪随机序列是相同的；所以并不能真正“随机”地打乱向量中的元素。伪随机问题不是《数据结构》研究的要点也不会考到；关于伪随机的一些讨论见《算法设计》部分。

直接看这个“向量置乱”的问题，很容易没有头绪。不妨将这个问题迁移到比较熟悉的领域：比如洗牌。

想必大家都非常熟悉洗牌。随机置乱的目的和洗牌是一样的，但如果用洗牌的方法去做随机置乱，即抽出一沓牌、把这沓牌放到牌堆底部、再抽一沓牌，则会面临三个问题：

1. 您不知道重复多少次抽牌比较合理；
2. 在有限次抽牌之后，牌的`n!`种随机次序并不是等概率的；
3. 每次抽牌都要伴随大量的元素移动，时间效率非常低下。

解决随机置乱问题可以从上面的第二个问题，也就是“<u>随机次序等概率</u>”入手。

为了保证随机次序是等概率的，那么就要构造`n!`种等可能的情况。根据乘法原理，可以很自然地想到，如果将每种次序表示为一个`n`元随机变量组$(X_1, X_2, ..., X_n)$，其中$X_i$两两独立，并且$X_i$恰好有`i`个等可能的取值，那么这`n!`种次序就是等可能的了。接下来，只需要建立在全排列和这样的`n`元组的一一对应的映射关系即可。

> 当然不能直接把全排列用上。全排列的两个元素不是相互独立的，它自身不是符合条件的`n`元组。

为了构造符合条件的映射，又可以采用递归的思想方法：

* 如果`n = 1`，全排列和`n`元组可以直接对应。
* 对于`n > 1`，考虑`V[n-1]`在打乱后的秩，显然，它可以取`0`、`1`、……、`n-1`这`n`个等可能的值，令这个数为$X_n$，然后将`V[n-1]`从打乱前后的向量中都删除，就化为了`n-1`的情况。反复利用这个化归方法，最终可化归到`n = 1`的情况。

以上就成功构造出了满足条件的一一映射关系，您可以在理解它的基础上自己设计相应的随机置乱算法。

```c++
// 算法2.8A
template <typename T>
void Vector<T>::shuffle() {
    for (Rank i = _size; i > 0; --i) {
        swap(_data[i-1], _data[rand() % i]);
    }
}
```

显然上面这个算法是时间$\Theta(n)$、空间$O(1)$的。并且上面的分析表明，如果`rand()`真的能随机生成一个非负整数（不是随机生成一个非负`int`！），那么**算法2.8**就能将所有的`n!`个排列等概率地输出。

> `rand()`如果随机生成一个非负`int`，那么`n`次`rand()`一共只有$2^{32n}=o(n!)$种可能的取值，所以在`n`充分大的时候，必然会有一些排列不可能被输出。
>
> 并且，不管这个`int`是多少位的，都不可能做到等概率输出。因为当`rand()`的返回值是在$[0,2^k-1]$中随机生成的非负整数时，`n!`在`n>=3`时不是$2^k$的因子（不论`k`有多大），所以这`n!`个排列不可能是等概率的。

### 偏序关系和全序关系

在讨论完置乱问题之后，接下来讨论排序问题。在具体介绍排序算法前，首先需要界定清除，**序**（order）是一个什么东西。在上一章定义过**良序**的概念，但要对一个向量做排序，并不一定要要求它的元素是某个定义了良序关系的类型。比如说，`n`个实数同样可以关于熟知的“≤”排序。

因此，需要引入条件更松的序关系的定义。

将良序关系定义中的第4个条件（最小值）去掉，就变成了**全序**（total order）关系。
$$
\mathbf{定义（全序关系）}\\
\begin{align}
&如果集合S上的一个关系\preceq满足：\\
&   1.（\mathbf{完全性}）x\preceq y 和 y \preceq x至少有一个成立。\\
&	2.（\mathbf{传递性}）如果x\preceq y且y\preceq z，那么x\preceq z。\\
&	3.（\mathbf{反对称性}）如果x\preceq y、y\preceq x均成立，那么x=y。\\
&那么称\preceq是S上的一个\mathbf{全序关系}，同时称S为\mathbf{全序集}。
\end{align}
$$
显然良序关系是全序关系的子集。

和良序关系相比，全序关系更加符合常规的认知。比如，实数集上熟知的“≤”就是全序关系。由于**完全性**的存在，凡是具有全序关系的数据类型，都可以进行排序；反之，在《数据结构》里的<u>通常意义的排序</u>问题中，都假定数据结构中的元素数据类型具有<u>先验的全序关系</u>。

> 在C++中，排序函数`sort`接受三个参数，其中第三个参数就表示“自定义的全序关系”。基本数据类型（如`int`和`double`）定义了内置的全序关系（即熟知的“≤”），但也可以使用其他的全序关系进行排序。详见《算法设计》。
>
> 其他编程语言中的排序函数也有类似的设计。

除了全序关系之外，还有一种序关系在《数据结构》中也经常会提到：**偏序**（partial order）关系。
$$
\mathbf{定义（偏序关系）}\\
\begin{align}
&如果集合S上的一个关系\preceq满足：\\
&   1.（\mathbf{自反性}）x\preceq x。\\
&	2.（\mathbf{传递性}）如果x\preceq y且y\preceq z，那么x\preceq z。\\
&	3.（\mathbf{反对称性}）如果x\preceq y、y\preceq x均成立，那么x=y。\\
&那么称\preceq是S上的一个\mathbf{偏序关系}，同时称S为\mathbf{偏序集}。
\end{align}
$$
偏序关系和全序关系相比，第1个条件（完全性）变成了更简单的自反性；也就是说，并不是$S$中的任意两个元素都能进行比较。

> 比如，令`S`为“考生组成的集合”，$\preceq$定义为“考生`x`的<u>每一门</u>分数都小于等于考生`y`”。您可以轻易验证，这个关系是偏序关系但不是全序关系。

在计算机编程中直接定义偏序关系是不方便的，因为$\preceq$的返回值往往是`bool`类型，不存在`true`和`false`之外的第三个选项（*无法比较*）。并且，*无法比较*的情况不能随意地返回一个`true`或`false`的值，因为这可能导致传递性被破坏。

所以，当在编程时需要定义一个偏序关系时，往往会将它扩展成一个全序关系。比如，给$S$中的所有元素做标号，当已有的偏序关系*无法比较*时，则根据标号的大小进行比较。

> 扩展成全序关系之后，就可以进行排序了。由扩展成的全序关系的不同，可能会产生不同的排序结果。

### 归并排序

现在回到向量排序的问题。

对于一个线性表，如果它的数据类型是全序的；且对其中的任意一个元素`x`，和`x`的<u>后缀</u>中的任意一个元素`y`，总是有`x`$\preceq$`y`，则称它是**有序的**（ordered）。对于无序线性表，通过移动元素位置使其变为有序的过程，称为**排序**（sort）。

> 在计算机领域所说的有序，一般都是指**升序**。所以在上面的定义中使用的是“后缀”。
>
> 如果您想要讨论降序或者其他的什么顺序（比如按最小素因子排序），只需要重新定义全序关系$\preceq$，即可以回归为升序的情况进行处理。

```c++
// 问题2.9 - 向量排序
// 给定：向量V[0:n]，全序关系cmp
// 要求：按照全序关系cmp对向量V做排序
```

排序是计算机领域最重要的算法之一。在计算机出现至今，人们提出了各种各样的排序算法，并且仍然有不少研究者在从事着排序算法的研究。在《数据结构》中，将专门有一章讨论各种排序算法。在本节，先介绍一种最基本、最经典的排序方法：**归并排序**（merge sort）。

> 归并排序的发明人是大名鼎鼎的冯·诺依曼，这位“计算机之父”在1945年设计并实现了该算法。

归并排序的设计采用的仍然是递归的思想：

* 规模`n <= 1`的向量总是天然有序的。

* 对于规模`n > 1`的向量，可以将其分成前后两部分，长度分别为`n/2`和`n-n/2`（您在上一章见过这个分法），从而将规模为`n`的问题化归为两个规模较小的子问题。这些子问题可以继续递归下去直到化为`1`。解决子问题之后，`V`的前半部分和后半部分分别有序，只需要将这`2`个有序序列合并为`1`个有序序列，就可以解决原问题了。这一合并的过程就称为**归并**（merge）。

您可以根据上面的思想，自己实现一个归并排序的算法，然后和下面的示例算法进行比较。

> 这个代码比较长。最好自己先写一份代码，因为直接读示例代码很难记住。
>
> 注意，在《数据结构》部分，代码的记忆既不是重点也没有必要。归并排序这个知识点的核心是上面的这一段文字：即**归并**的思想。

```c++
// 算法2.9A
// 其中，B是一个辅助数组
template <typename T> /* ...省略中间内容，详见配套代码... */
void mergeSort(T* A, int n, function<bool(const T&, const T&)> cmp) {
    if (n <= 1) { return; }        // 递归边界
    int m = n / 2;                 // 取中点
    mergeSort(A, m, cmp);          // 递归地排序前半部分
    mergeSort(A+m, n-m, cmp);      // 递归地排序后半部分
    arrayCopy(B, A, m);            // 开始归并，将前半部分复制到辅助数组，后半无需复制
    T* A1 = B, * A2 = A+m;         // 前半部分记为A1，后半部分记为A2
    int L1 = m, L2 = n - m;        // A1和A2的长度
    int i = 0, j = 0, k = 0;
    while (j < L1 && k < L2) {     // 在两个部分的元素都没有用尽时
        if (cmp(A1[j], A2[k])) {   // 比较它们还未加入A的最小的元素
            A[i++] = A1[j++];      // 将二者的较小者加入到A中
        } else {
            A[i++] = A2[k++];
        }
    }
    while (j < L1) {               // 如果A1还有多余元素没加入A
        A[i++] = A1[j++];          // 就将剩余元素加入A
    }
}

template <typename T>
void Vector<T>::mergeSort(function<bool(const T&, const T&)> cmp) {
    MergeSort<T> sort;
    sort(_data, _size, cmp);
}
```

> 由于向量被封装了起来，这里没有直接在向量层次做递归，而是在<u>向量所基于的数组</u>层次做。这个编程技巧和《数据结构》的内容没有什么关系。

在阅读并理解上述归并排序的算法的基础上，您可以回答以下问题（其中，假定比较函数`cmp`的时间、空间复杂度都是$O(1)$的）：

1. 为什么向量的前半部分必须要复制到辅助数组去，而后半部分不需要？

   > 因为如果前半部分不复制的话，一旦后半部分比前半部分的元素小，就会把前半部分的元素覆盖掉；
   >
   > 而对后半部分而言，由于`A2[k]`就是`A[m+k]`，所以除非`A1`的元素已经全部加入`A`，否则`i`永远追不上`m+k`。因此，后半部分是不存在覆盖问题的。

2. 为什么没有讨论后半段`A2`有多余元素的情况？

   > 因为`A2`的数据没有复制出去，如果`A1`的元素已经全加入到`A`了，则`A2`剩余的元素已经在它们应该在的位置上，不需要再移动了。

3. 辅助数组`B`的长度至少是多少？并由此确定**算法2.9A**的空间复杂度。

   > 辅助数组`B`的长度至少为最大的`m`，也就是`n/2`，因此空间复杂度为$\Theta(n)$。
   >
   > 注：递归产生的$\Theta(\log n)$，相比于辅助数组的$\Theta(n)$来说可以忽略；但<u>在解答题的场合不可忽略，必须书写在解题过程中</u>。

4. **算法2.9A**的时间复杂度是多少？如果每次划分的时候`m`不取`n/2`而是取`kn`（其中`0<k<1`），时间复杂度又会变成多少？如果`m`取`max(n/2, C)`，其中`C`是一个给定的常数，那么时间复杂度又会变成多少？

   > 这个问题是归并排序相关的一个经典问题，考试中也可能出现。
   >
   > 对于原始的**算法2.9A**（**折半二分**），您可以列出$T(n)=2T(\frac n2)+\Theta(n)$的方程，递降计算出$T(n)=\Theta(n\log n)$。当`m`取`kn`（**定比二分**）时做法类似，时间复杂度不会变，但常数会增加（可以令上面方程中的$\Theta(n)=n$，以方便讨论常数）。
   >
   > 如果`m`存在上限`C`（**定长二分**），则在`n`充分大时，$T(n)=T(n-C)+T(C)+\Theta(n)=\Theta(n^2)$。
   >
   > 因此`m`必须按比例取，而不能受到某个固定值`C`的限制。

5. 如果向量已经基本有序，只有开头的长度为`L`的一小段前缀是乱序的（即前缀外全部有序，且前缀中的元素都比前缀外的元素小），如何改进**算法2.9A**，让它可以有更高的时间效率？改进之后的时间复杂度是多少？

   > 这个问题也是一个排序经典问题，考试中可能出现。
   >
   > 首先容易证明，原有的**算法2.9A**在最好情况下，时间复杂度也是$\Theta(n\log n)$的。所以必须要改进。
   >
   > 改进的时候，可以针对已知的方程$T(n)=2T(\frac n2)+\Theta(n)$做优化。这个方程中，递归项$2T(\frac n2)$只要不改动递归方式，就是没法做优化的；而余项$\Theta(n)$是有机会被优化的。需要在“<u>比较好的情况</u>”（即题中给出的“基本有序”的情况）下，让$\Theta(n)$变得更小。
   >
   > 就这个问题而言，只需要在归并前增加一行：
   >
   > ```c++
   > if (A[m] <= A[m+1]) { return; }
   > ```
   >
   > 这样，如果归并前的序列已经有序，就不需要进行归并。
   >
   > 那么，不需要归并的部分就可以降到$T(n) = 2T(\frac n2) + O(1)$，也就是$\Theta(n)$。
   >
   > 需要归并的部分由题意，长度不超过`2L`，在这部分利用上一题中获得的结论，就可以得到时间复杂度为$\Theta(L\log L)$。
   >
   > 因此，改进后的时间复杂度为$\Theta(n+L\log L)$。

上述问题中4、5两题是典型的排序问题的命题角度。在后面的章节中介绍到其他的排序时，还将再次从这个角度进行观察。

### 基于比较的排序的时间复杂度

归并排序是一种**基于比较**（comparison-based）的排序。

> 所谓基于比较，就是在算法进行过程的每一步，都依赖于元素的比较（即调用`cmp`）进行。
>
> 基于比较的排序是针对**全序关系**设计的。大多数的排序算法都是基于比较的。
>
> 还有一些不基于比较的排序，它们不是针对待排序数据类型的**全序性**设计的，而是针对待排序数据类型的其他性质设计的，因而应用范围会更小。在后文中会介绍一些不基于比较的排序。

下面将证明一个重要结论：
$$
\mathbf{定理}\\
基于比较的排序，在最坏情况下的时间复杂度必定是Ω(n\log n)。
$$
这是本笔记中第一次使用信息论方法，讨论时间复杂度的**最优性**。信息论方法在考研试卷上通常不会直接考到，但用信息论的思路，有助于在算法设计题中判断自己是否能得到时间复杂度层面上的满分。

* 在排序算法开始之前，这`n`个元素可能的顺序关系有`n!`种，而在排序算法开始之后，这`n`个元素可能的顺序关系只有`1`种（因为已经找到了它们的顺序）。

  > 在最坏情况下，`n`个元素互不相同，排序后的顺序关系是确定的。

* 另一方面，每次比较都有两种结果（`if`分支和`else`分支）。剩下的可能的顺序关系被分为`2`个部分，根据比较结果，只保留其中的`1`个部分。

  > 在最坏情况下，每次保留的都是元素较多的部分，从而每次比较至多排除一半的可能。

综合以上两点，至少需要进行$\log_2(n!)=\Theta(n\log n)$次比较。

这个结论基于一个重要的公式：
$$
\mathbf{定理（Stirling公式）}\\
在n\to\infty时，n!\sim \sqrt{2\pi n}\cdot \left(\frac ne\right)^n。
$$

> 这一公式的证明和计算机考研无关。感兴趣的话可自行在网上查找，这里不再叙述。
>
> 在《数据结构》的学习中需要记住的公式并不多，Stirling公式是必须记住的公式之一。或者，您也可以只记住$\log(n!)=\Theta(n\log n)$，因为这是Stirling公式在《数据结构》考研中的主要应用。

由此可见，归并排序在基于比较的排序中，已经达到了最优的时间复杂度。当然，空间复杂度不是最优的，它需要$\Theta(n)$的额外空间。关于排序的更多性质，在后面的专门章节中将继续分析。

### 折半查找

在介绍完排序之后，来看一下排序之后得到的有序向量，在查找时有什么额外的优越性。

执行查找操作的时候，不再需要一个一个元素看是否相等了。这里可以使用刚才介绍的基于比较的算法思路。

将被查找的元素`e`和向量中的某个元素`V[i]`比较，比较结果有2种：

* 如果`V[i] > e`，那么只需要保留`V[0:i]`作为新的查找区间；
* 如果`V[i] <= e`，那么只需要保留`V[i:n]`作为新的查找区间。

当取`i = n/2`（**折半二分**）时，可以保证新的查找区间长度大约是原来的一半。所以这个思路称为**折半查找**。您可以尝试设计折半查找的算法。

```c++
// 问题2.10 - 向量查找
// 给定：向量V[0:n]，全序关系cmp
// 输入：待查找的元素e
// 输出：元素e在向量V中的秩
//      如果有多个满足条件的元素，输出最大的秩
//      如果e在V中不存在，输出-1
```

> 查找是算法设计题的考试重点，在设计的时候，需要尤其注意多个相等元素的时候是返回秩最大、秩最小还是任意一个，以及查找失败的时候返回何种特殊值。
>
> 上面的问题描述中，最后两行就是针对“多个结果”和“没有结果”情况的输出规定。

和归并排序一样，笔者建议您首先自己写一个答案。

```c++
// 算法2.10A
template <typename T> /* ...省略中间内容，详见配套代码... */
int binarySearch(T* A, int n, T e, function<bool(const T&, const T&)> cmp) {
    int i = n / 2;                   // 折半
    if (n == 1) {                    // 递归边界
        return A[0] == e ? 0 : -1;
    }
    if (!cmp(A[i], e)) {
        return binarySearch(A, i, e);      // 递归进入左半部分
    } else {
        int r = binarySearch(A+i, n-i, e); // 递归进入右半部分
        return r < 0 ? r : r + i;    // 复原在整个数组中的下标
    }
}

template <typename T>
Rank Vector<T>::binarySearch(T e, function<bool(const T&, const T&)> cmp) {
    BinarySearch<T> search;
    return search(_data, _size, e, cmp);
}
```

**算法2.10A**的时间复杂度和空间复杂度均为最坏$\Theta(\log n)$，请您自己证明这一点。

### 消除简单尾递归

查找`V[0:n]`中某个元素`e`的下标，这个问题在计算前有`n+1`种（包括`-1`）可能的结果，计算后有`1`种确定的答案，因此最坏时间复杂度一定是$\Omega(\log n)$的。但空间复杂度并不一定要是$\Omega (\log n)$。在这一小节，将介绍一种叫做**消除尾递归**的技术，使用这个技术，可以将**算法2.10A**的空间复杂度降为$O(1)$。

如果一个递归函数只在**返回**（return）前调用自身，则称其为**尾递归**（tail recursion）。特别地，如果在返回前只调用自身至多一次，则称为**简单尾递归**。在本小节，只介绍对于简单尾递归的消除方法。

对于简单尾递归，只需要将递归函数的参数作为循环变量，就可以将其改写为不含递归的形式，从而降低空间复杂度。

```c++
// 包含尾递归的原函数
ReturnType Function(ParameterType Parameter) {
    if (Condition(Parameter)) { // 递归边界
        return BoundaryValue(Parameter);
    }
    // ...（函数体）
    return Function(NextParameter(Parameter)); // 尾递归
}
```

上面是一个简单尾递归的一般模型。将递归参数`Parameter`改为循环变量，则可以变成：

```c++
// 不含尾递归的新函数
ReturnType Function(ParameterType Parameter) {
    ParameterType p = Parameter;
    while (!Condition(p)) {
        // ...（函数体）
        p = NextParameter(p); // 被消除的尾递归
    }
    return BoundaryValue(p);
}
```

这个方法在**算法2.10A**上不能直接应用，因为返回时可能会加上一个偏置量（`i`）。不过，因为加法具有结合律，所以只需要增加一个循环变量维护<u>累计偏置量</u>就可以了。您可以自己利用上述方法来尝试将**算法2.10A**的空间复杂度优化到$O(1)$，下面是一个改写后的例子。

```c++
// 算法2.10B
int binarySearch(T* A, int n, T e, function<bool(const T&, const T&)> cmp) {
    int i = 0;                 // 维护累计偏置量
    while (n > 1) {            // 判断是否到达递归边界
        int m = n / 2;         // 折半
        if (!cmp(A[m], e)) {
            n = m;             // 递归进入左半部分
        } else {
            i += m;
            A += m;            // 递归进入右半部分
            n -= m;
        }
    }
    return A[0] == e ? i : -1; // 递归边界
}
```

> 当“没有结果”的时候，返回无效秩`-1`是一个比较自然的想法。
>
> 但是有的时候需要返回一个其他的数值。比如说，要向有序向量`V`中插入一个新元素`e`，但不希望破坏它的有序性。那么，就必须找到一个<u>合理的插入位置</u>。
>
> 在**算法2.10B**中，最后一行改为直接`return i`，那么就可以返回到一个合理的插入位置：新元素`e`一定应当被插入在`V[i]`<u>之前或之后</u>。请注意**算法2.10B**直接进行这样的修改，并不能判断出应当插入在之前还是之后，还需要对`V[i]`和`e`的大小进行比较；您可以做出适当的调整，让它能够直接返回合理的插入位置。

现代编译器通常可以在编译的过程中自动消除简单尾递归，所以在实际上机编程时，不需要刻意将简单尾递归改写成循环形式。但在《数据结构》学科分析算法的时候，不应该考虑编译器做的优化，所以在算法设计题中，如果出现了未被改写的简单尾递归，几乎一定是会被扣分的。

### 向量唯一化

下面讨论一下唯一化问题。

```c++
// 问题2.11 - 向量唯一化
// 给定：向量V[0:n]
// 要求：将V中的重复元素删去，只保留秩最小的那一个
```

一个简单但有效的解法是：从左到右考察`V`中的每个元素，如果它在自己的前缀中已经出现过了，那么就把这个元素删除。同样，在看下面的代码前，您可以自己实现这个算法。

```c++
// 算法2.11A
template <typename T>
void Vector<T>::duplicate() {
    Rank i = 1, k = 1;                  // 快指针i检索，慢指针k填充
    while (i < _size) {                 // V[0:k]始终是V[0:i]唯一化后的结果
        bool existInPrefix = false;     // 要判断V[i]是否在V[0:i]中
        for (Rank j = 0; j < k; ++j) {  // 只需要判断是否在V[0:k]中即可
            if (_data[i] == _data[j]) {
                existInPrefix = true; break;
            }
        }
        if (existInPrefix) {            // 如果V[i]不在它的前缀中
            _data[k++] = _data[i++];    // 移动元素，并同时移动快慢指针
        } else { ++i; }                 // 否则只需要移动快指针
    }
    _size = k;                          // 修改规模，和慢指针对齐
    shrink();                           // 如果有必要，则缩容
}
```

最坏情况是所有`V`中的元素互不相等的情况。为了验证所有元素互不相等，必须要进行至少$\Theta(n^2)$次比较。所以**算法2.11A**的最坏时间复杂度是$\Theta(n^2)$。

而最好情况是所有`V`中元素全部相等的情况，最好时间复杂度是$\Theta(n)$。

> 这个最好时间复杂度得益于在删除元素时使用了**算法2.7C**引入的快慢指针。如果像**算法2.7A**和**算法2.7B**一样每次都使用`remove`进行删除，那么最好情况下也需要$\Theta(n^2)$的时间。

在有序向量的情况下，相等的元素总是排在连续的位置的。所以，要讨论一个元素是否在它的前缀中可以找到，只需要比较它是否和自己的直接前驱相等即可。

根据这一思路，可以对**算法2.11A**进行改进：只需要将`existInPrefix`的计算过程简化为`existInPrefix = _data[i] == _data[k-1]`。改进后的算法，时间复杂度是$\Theta(n)$。

> 如果定义了全序关系（即可排序），那么无序向量的唯一化可以化归到有序向量的情况进行处理：先进行一次$\Theta(n\log n)$的排序，再用有序向量唯一化。但在排序的时候，会损失“元素原先的位置”这一信息，所以需要开辟一个额外的$\Theta(n)$的空间保存这一信息，以在唯一化之后能够顺利还原。

### 循环位移

在本节的最后，用**循环位移**（cyclic shift）作为例子，讨论一下非常规的遍历方法。

给定向量`V[0:n]`和位移量`k`，则将原有的向量`V[0],V[1],...,V[n-1]`，变换为`V[k],V[k+1],...,V[n-1],V[0],V[1],...,V[k-1]`，称为**循环左移**。相应地，变换为`V[n-k],V[n-k+1],...,V[n-1],V[0],V[1],...,V[n-k-1]`，称为**循环右移**。循环左移和循环右移的实现方法大同小异，这一小节只讨论循环左移，右移的情况请您自己完成。

```c++
// 问题2.12 - 循环位移
// 给定：向量V[0:n]，位移量k（0<k<n）
// 要求：将向量V循环左移k个单位
```

相信您一定知道最经典的**交换**函数`swap`的实现：

```c++
t = a, a = b, b = t;
```

一个最朴素的想法，就是用类似的辅助空间，暂存`V[0:k]`中的元素，然后通过3次赋值（实际上是数组拷贝）来完成循环左移。

```c++
// 算法2.12A
template<typename T>
void cyclicLeftShiftA(const Vector<T>& V, int k) {
    T* B = new T[k], * A = V.data();
    int n = V.size();
    arrayCopy(B, A, k);     // B[0:k] = V[0:k]
    arrayCopy(A, A+k, n-k); // V[0:n-k] = V[k:n]
    arrayCopy(A+n-k, B, k); // V[n-k:n] = B[0:k]
}
```

这一算法的时间复杂度是$\Theta(k)+\Theta(n-k)+\Theta(k)=\Theta(n+k)$。考虑到$k=O(n)$，也可以简化为$\Theta(n)$。空间复杂度则为$\Theta(k)$。

下面的目标则是将空间复杂度降到$O(1)$。

为了保持时间复杂度仍然为$\Theta(n)$不变，需要尽可能“一步到位”地移动元素。现在让`V[i+k]`移动到`V[i]`的位置上，暂存`V[i]`到辅助空间`temp`。下一步，如果继续将`V[i+k+1]`移动到`V[i+1]`的位置，那么需要的辅助空间就会增大。为了防止辅助空间增大，则需要考虑“不用暂存”的元素：也就是已经被移动的`V[i+k]`。下一步将`V[i+2k]`移动到`V[i+k]`的位置，这就是不需要新的辅助空间的。

因此可以得到一个算法：将`V[i+k]`移动到`V[i]`，再将`V[i+2k]`移动到`V[i+k]`，以此类推。由于向量中的元素是有限的，您可以证明，存在`j`，使得`(i+jk) % n = i`，也就是经过`j`次移动后回到了`V[i]`。最后一次赋值，将辅助空间里的`V[i]`拿出来赋给`V[i-k]`也就是`V[i+(j-1)k]`即可。

需要注意的是，这样一轮并不一定能经过`V`中所有的元素。比如在`n=6, k=2, i=0`时，只轮转交换了`V[0],V[2],V[4]`这3个元素，而对另外3个元素则没有移动。

下面证明：对任意的秩`0 <= r < n `，都存在唯一的`0 <= i < d`和`0 <= j < n/d`，使得`r = (i+jk) % n`。其中，`d = gcd(n,k)`是`n`和`k`的最大公约数。

> 因为`r`和`(i,j)`的取值范围都是`n`元集，所以只要证明`(i,j)`到`r`是单射，就蕴含了它同时是满射。
>
> 因此，只需要证明对于不等的$(i_1,j_1)$和$(i_2,j_2)$，$(i_1+j_1k) \%n\ne(i_2+j_2k)\%n$。
>
> 假设存在整数$q$，使得$(i_1-i_2)+(j_1-j_2)k+qn=0$。由于$(j_1-j_2)k+qn$必定是$d$的倍数，而$|i_1-i_2|<d$，所以只能有$i_1=i_2$。
>
> 设$k=k_1d,n=n_1d$，那么$(j_1-j_2)k_1+qn_1=0$。因为$(k_1,n_1)=1$，所以$j_1-j_2$必定是$n_1$的倍数，但$|j_1-j_2|<n/d=n_1$，所以只能有$j_1=j_2$。这和$(i_1,j_1)$与$(i_2,j_2)$不等矛盾，故由反证法得到单射成立。

于是，遍历顺序应当是：依次从`0`、`1`、……、`d-1`出发，以`k`为步长遍历`n/d`次回到起点。

```c++
// 算法2.12B
template<typename T>
void cyclicRightShiftB(const Vector<T>& V, int k) {
    auto A = V.data(), n = V.size();
    int d = gcd(n, k), n1 = n / d;      // 计算最大公约数
    T temp;                             // 辅助空间
    for (int i = 0; i < d; ++i) {       // 外层循环
        temp = A[i];                    // 放入辅助空间
        for (int j = 0; j < n1-1; ++j) {  // 内层循环，以k为步长位移
            A[(i+j*k) % n] = A[(i+(j+1)*k) % n];
        }
        A[(i+(n1-1)*k) % n] = temp;       // 最后一步，从辅助空间取出
    }
}
```

> 为了和上面的分析保持一致，这里直接用`(i,j)`作为循环变量。
>
> 实际上，可以用`(i,i+j*k)`作为循环变量，以降低`j*k`乘法运算的次数。您可以自己改写这个算法。

这一算法的时间复杂度是$\Theta(d)\cdot \Theta(n/d) = \Theta(n)$，空间复杂度降低到了$O(1)$。

在很多教材上会介绍另一种解法。

```c++
// 算法2.12C
template <typename T>
void cyclicRightShiftC(const Vector<T>& V, int k) {
    auto A = V.data(), n = V.size();
    reverse(A, k);   // -> rV[0:k] + V[k:n]
    reverse(A, n);   // -> rV[k:n] + V[0:k]
    reverse(A, n-k); // -> V[k:n] + V[0:k]
}
```

其中`reverse`表示将数组颠倒过来。

```c++
template <typename T>
void reverse(T* A, int n) {
    for (int i = 0; i < n/2; ++i) {
        swap(A[i],A[n-1-i]);
    }
}
```

容易证明，**算法2.12C**和**算法2.12B**具有相同的时间复杂度和空间复杂度。但是，**算法2.12B**的赋值次数是$d(n_1+1)=n+d\le \frac 32 n$，而**算法2.12C**的赋值次数是$\frac32 \cdot(k+n+(n-k))=3n$，所以**算法2.12B**的常数是比**算法2.12C**更小的。

> 这里只讨论数组元素的赋值，而不讨论循环变量`i`等的赋值。
>
> 这是因为在计算机中，循环变量总是存在寄存器里，而数组元素存在内存中，二者的读写速度相差很大。关于这个问题，详见《组成原理》部分。

## 列表

在前面两节介绍了线性表的一种：向量，在这一节介绍另一种线性表：**列表**（list）。

列表和向量的区别在于，<u>列表不要求在内存中占据的空间是连续的</u>。

这一特点赋予了列表更大的灵活性，使列表的一些操作比向量效率更高；但同时，这一特点也让列表无法通过秩定位到内存地址，丧失了循秩访问的能力，从而在另一些操作上效率不如向量。这篇笔记会介绍列表是如何工作的，至于<u>列表和向量的对比表</u>，希望读者在阅读后自己完成。

### 列表的链式实现

因为内存地址不再能反映元素在列表中的位置，所以列表需要通过其他方法指示元素的位置。这种方法不是唯一的。在笔记中将主要介绍考研初试的重点：链式实现。

> 初试基本上只会考链式实现。但在复试上机的时候，经常不使用链式实现；因为链式实现需要动态分配内存，从而影响算法的执行时间。详见《算法设计》。

和向量一样，先给出实现的基本框架。

```c++
template <typename T>
struct ListNode {       // 列表中的一个数据单元（节点）
    T value;            // 本节点存放的数据
    ListNode<T>* pred;  // 指向列表中的直接前驱
    ListNode<T>* succ;  // 指向列表中的直接后继
};

template <typename T>
class List {
private:
    int _size;          // 列表的规模
    ListNode<T>* _head; // 列表的头哨兵节点
    ListNode<T>* _tail; // 列表的尾哨兵节点
};
```

列表中的每个数据单元称为一个**节点**（node）。

> 在一些教材中使用的翻译是结点，二者没有意义上的差别。
>
> 作为线性表，可以在列表中和向量一样定义前驱节点、后继节点等概念。

在每个节点中，除了记录存放的数据，还需要说明这个节点<u>在列表中处于何种位置</u>。解决这一问题的方案是：在节点里保存它的直接前驱节点和直接后继节点的指针。这样，当需要访问一个节点的前驱时，只要一路沿着`pred`向前，访问后继时只要一路沿着`succ`向后就可以了。和向量的**循秩访问**对应，这种从一个节点出发，根据相邻节点的位置关系进行访问的做法称为**循位置访问**。

> 在一些教材中使用的直接前驱/直接后继变量名是`prior`和`next`，词汇的不同属于英语问题，理解即可，不再赘述。

列表由若干个节点组成，但在列表类`List`中并不需要保存所有的节点的位置，因为反正也没法循秩访问，记录了所有的节点也用不到。

在`List`类中，只需要保存<u>列表的第一个节点和最后一个节点的位置</u>就可以了。在本笔记的实现中，使用了**头哨兵节点**和**尾哨兵节点**（简称头节点和尾节点）这两个不保存实际数据的节点，用来标志<u>列表的起始和结束</u>。即使列表是空列表，这两个节点也存在。

> 在一些教材中介绍的列表是没有哨兵节点的，这不会影响到列表的功能，但有些功能的实现会更加复杂。在阅读完本节之后，您可以自己尝试写一个没有哨兵节点的列表，比较一下哪些功能的实现有所区别。

空向量只需要让`_data = nullptr, _size = _capacity = 0`即可，但由于哨兵节点的存在，空列表的初始化会稍微复杂一些。

```c++
template <typename T>
List<T>::List() {
    _size = 0;
    _head = new ListNode<T>();
    _tail = new ListNode<T>();
    _head->succ = _tail; _head->pred = nullptr;
    _tail->pred = _head; _tail->succ = nullptr; 
}
```

在初始化之后，头节点和尾节点通过一组`succ`/`pred`相连接，而头节点的直接前驱和尾节点的直接后继是没有定义的，因此赋值为空指针`nullptr`。列表中的数据将会被插入到头节点和尾节点之间，如下图所示。

![链表](..\..\pic\ds\链表.png)

可以看到，在这个结构中，列表中的每个节点像是被`pred`和`succ`两条锁链串起来了一样，因此列表的这种实现方式也被称为**双链表**。如果只保留头节点和`succ`指针，去掉尾节点和`pred`指针，那么就只能从一个节点向后访问，而无法向前访问了，这种实现称为**单链表**。因为单链表比双链表简单，理解双链表也就理解了单链表，所以笔者不会为它花费多余的篇幅。

> 在考研中有可能会考到单链表，您可以在阅读完本节之后自行实现单链表。您会发现有一些功能用单链表会很不方便实现。

可以看出，如果要用秩`r`对列表进行“循秩访问”`L[r]`，则从`head`开始，沿着`succ`迭代`r`次，或者从`tail`开始，沿着`pred`迭代`n-r`次。

```c++
template <typename T>
ListNode<T>* List<T>::operator[](Rank index) {
    ListNode<T>* p = nullptr;
    if (index <= _size / 2) {
        for (p = _head; index >= 0; --index) {
            p = p->succ;
        }
    } else {
        for (p = _tail; index < _size; ++index) {
            p = p->pred;
        }
    }
    return p;
}
```

所以“循秩访问”的时间复杂度为$\Theta(\min(r,n-r))=O(n)$。当`r`等概率分布时，平均时间复杂度为$\Theta(n)$。因此，列表的“循秩访问”是十分低效的，在将向量算法改写成列表形式时，应当尽可能避免循秩访问，改为循位置访问的形式。

### 插入节点

这一小节开始依次讨论列表的插入、删除和查找。

```c++
// 问题2.13 - 列表插入元素（前插）
// 给定：列表L
// 输入：被插节点p，待插入的元素e
// 要求：将元素e插入到列表L中，它所在的节点成为p的直接前驱
```

在解决列表有关的问题时，可以绘制下面这样的链式图。它可以使您更容易理清思路，避免遗漏对某条链子的赋值。

![链表插入](..\..\pic\ds\链表插入.png)

上图展示了新节点`x`是如何被插入到`p`前的，其中`q`表示`p`原来的直接前驱。`x`关联了`4`条`pred`/`succ`链子，因此在写插入函数时，应该有`4`条赋值语句。

```c++
// 算法2.13A
template <typename T>
void List<T>::insertAsPred(ListNode<T>* p, T e) {
    auto q = p->pred, x = new ListNode<T>();
    x->value = e;                   // 创建新节点
    x->pred = q; x->succ = p;       // 将新节点的链子挂在p和q之间
    q->succ = x; p->pred = x;       // 断开p和q之间的两条链子
    ++_size;                        // 更新列表规模
}
```

前插操作的时间复杂度为$O(1)$，这得益于`pred`指针，它使得`p`的直接前驱可以被直接定位。

> 如果是单链表的情形，则必须要从头开始一路向后找到`p`，才能确定`p`的直接前驱，此时时间复杂度为$\Theta(r)$，其中`r`为节点`p`的秩。

有了前插，自然也就有后插。因为笔记中的实现定义了尾节点，所以列表中的数据节点的`succ`总不为空，从而后插可以化归为“直接后继的前插”。您也可以模仿前插，自己写出后插的算法。

> 在单链表中，因为`succ`可以直接定位，所以后插复杂度可以做到$O(1)$。

既然在单链表中后插复杂度远小于前插，那么是否可以用后插实现前插呢？在一些情况下是可以的。为了实现对`p`的前插，您只需要复制一个`p`插入到原来的`p`后面，然后将原来的`p`的`value`改成被插入的数据即可。这样就好像后插的节点副本是被插节点，而修改后的原节点是插入的节点一样。

```c++
// 算法2.13B（单链表前插）
template <typename T>
void List<T>::insertAsPred(ListNode<T>* p, T e) {
    auto x = new ListNode<T>();
    x->value = p->value;            // 复制节点p
    x->succ = p->succ; p->succ = x; // 后插，复制得到的x当做被插节点
    p->value = e;                   // 而原来的p当做插入的节点
    ++_size;                        // 更新列表规模
}
```

**算法2.13B**使得单链表前插的时间复杂度也降到了$O(1)$。但在某些列表实现中，节点的`value`只能在创建时赋值，之后不能修改（被设置成了只读变量），这种情况就无法应用**算法2.13B**。

在另一些情况下，`value`是非常大的数据块。这种情况下，**算法2.13B**需要额外进行一次`value`的赋值；在链表步长的情况下，其造成的时间消耗可能比从`head`沿着`succ`找过来更高。这种情况也不适合使用**算法2.13B**。

> 笔者举这两个例子，希望您能意识到<u>您学习的算法总是有它的适用范围</u>。您在面对实际问题时应该关注题目设定的条件，判断是否能应用自己学过的算法解决。

### 删除节点

列表删除是列表插入的逆操作，您可以对照在上一小节中的图，从右到左看，去实现删除。可以看出，只要重新赋值`2`条链子即可。

```c++
// 问题2.14 - 列表删除元素
// 给定：列表L
// 输入：被删除的节点p
// 要求：将节点p从列表L中删除
```

注意在C/C++中，一般需要显式地释放被删除节点的内存。

> 确实，使用智能指针可以不显式释放内存。但是使用智能指针并且需要占用更多的空间。对于基本数据结构而言，通常是回避使用智能指针的。

```c++
// 算法2.14A
template <typename T>
void List<T>::remove(ListNode<T>* p) {
    p->pred->succ = p->succ;
    p->succ->pred = p->pred;
    --_size;
    delete p;  // 显式释放内存
}
```

删除操作的时间复杂度是$O(1)$。

### 查找元素

```c++
// 问题2.15 - 列表查找
// 给定：列表L
// 输入：待查找的元素e
// 输出：元素e在列表V中的位置
//      如果有多个结果，返回最后一个位置
//      如果没有结果，返回nullptr
```

列表的查找和向量一样，只需要沿着某个方向检索一遍即可。

```c++
template <typename T>
ListNode<T>* List<T>::find(T e) {
    for (auto p = _tail->pred; p != _head; p = p->pred) {
        if (p->value == e) { return p; } // 从后向前检索
    }
    return nullptr;
}
```

和向量一样，列表查找的时间复杂度同样是$\Theta(n-r)=O(n)$，平均复杂度$\Theta(n)$。

您可以对**算法2.15A**稍作修改，将其改为“多个结果返回最前的节点”和“不能用`tail`/`pred`的单链表情况”。

> 此外，同样因为不支持循秩访问的缘故，即使是有序列表，也没法使用折半查找。

### 归并排序

从上面的几个小节可以看出，基础的三大操作中，向量的规模`_size`没有发挥任何作用。所以，如果只用来实现基础操作，列表是可以不记录这个属性的。但要做归并排序，就需要用到它了。

您应当根据向量的归并排序自行写出列表的归并排序。在向量的对应小节讨论了归并排序的几个注意点（后面的题目1、2、3），在列表的场合同样需要注意到它。

```c++
// 算法2.16A
template <typename T>              // 从s到t（不包含t）的n个节点进行排序
void mergeSort(ListNode<T>* s, ListNode<T>* t, int n, function<bool(const T&, const T&)> cmp) {
    if (n <= 1) { return; }        // 递归边界
    int m = n / 2;                 // 取中点
    auto mid = s->forward(m);      // 取m次succ，找到列表的中点
    mergeSort(s, mid, m);          // 递归地排序列表的前半部分
    mergeSort(mid, t, n-m);        // 递归地排序列表的后半部分
    auto p = s, q = mid;           // p和q作为两部分的指针
    while (p != mid && q != t) {
        if (cmp(p->value, q->value)) {
            p = p->succ;           // 前半小，不动
        } else {
            q = q->succ;           // 后半小，移到前面去
            T e = q->pred->value;  // 被移动的节点
            remove(q->pred);       // 从q的前面删除
            insertAsPred(e, p);    // 插入到p前面去
        }
    }
}

template <typename T>
void List<T>::mergeSort(function<bool(const T&, const T&)> cmp) {
    mergeSort(_head->succ, _tail, _size, cmp);
}
```

> `forward(m)`表示连续取`m`次`succ`得到的节点，具体实现见配套代码。

复杂度分析：

* 尽管花了$\Theta(m)=\Theta(n)$的时间定位中点，但本来归并就需要$\Theta(n)$，所以时间复杂度和向量版本相同。当然常数远高于向量。
* 因为列表的插入和删除都是在链子上做操作，没有对`value`的赋值，所以不需要考虑数据被覆盖的问题。因此，列表形式的空间复杂度是$\Theta(\log n)$，不需要辅助数组。

在向量对应小节讨论的问题4、5，列表的场合也有类似的结论，您可以自己讨论它们。

### 循环链表

**循环链表**（circular list）是链式列表的一个变体。

简单地说，循环链表就是舍弃`head`和`tail`，让`L[0]->pred`指向`L[n-1]`，而`L[n-1]->succ`指向`L[0]`，从而形成一个链环的结构。循环链表同样可以分为循环单链表和循环双链表。

> 循环链表的三种基本操作和本节中介绍的普通链表几乎完全一致，所以不再赘述。

循环链表的主要特征是：循环链表上的指针沿着`succ`或`pred`的方向走下去，就可以在链表上**无限轮询**每个节点。这种无限轮询的特点使它在《网络原理》和《操作系统》这两门学科“持续性维护网络（操作系统）”的情景中都有闪亮登场，但在《数据结构》学科“编程在有限时间内算出结果”的情景下不常用。

### 列表的静态实现

**静态链表**（static list）指的是基于数组实现的列表，在提前预知列表的规模上限的时候可以用到它。

> 或者也可以基于向量实现，从而允许扩容。

和常见的链式实现的列表相比，静态链表需要消耗更多的空间，所以它非常不常用，也几乎不会出现在初试中。但在复试上机的时候，由于动态分配内存可能会引起时间消耗上的不确定性，并且机试题目给定的空间几乎总是绰绰有余的，所以静态链表经常被用到。

> 和普通的链式实现相比，基于数组的静态链表保证了列表中的节点在内存中的地址是连续的一整块，从而可以更好地利用硬件的高速缓存机制（见《组成原理》）加快访问速度。这对于机试是有一定价值的。
>
> 其他带“链”的结构，如后面几章讨论的二叉树，在机试中也经常基于数组去“静态实现”，不再赘述，留到《算法设计》里再进行讨论。

单链/双链、线性/循环、链式/静态，这三对关系是相互独立的，可以组成`8`种不同结构的列表实现。在本节中需要介绍的是双链、线性、链式的列表，下面以双链、线性、静态的列表为例，介绍静态链表的实现方法。为了支持扩容，这里基于向量实现。

```c++
template <typename T>
class StaticList {
private:
    int _size;
    const Rank _head = 0;
    const Rank _tail = 1;
    Vector<StaticListNode<T>> V;
public:
    StaticListNode<T>* get(Rank r) { return &V[r]; }
};
```

将所有的静态列表中的节点，都放在一个向量`V`里，并规定`V[0]`和`V[1]`分别是头部和尾部的哨兵。

> `get`方法表示在`V`中循秩访问。因为列表中的元素次序和`V`中的元素次序并不相同，所以它的语义和直接对列表循秩访问`L[r]`是不同的。

因为所有的节点都在向量`V`中，所以`StaticListNode`中不需要保存直接前驱和后继的<u>指针</u>，只需要保存直接前驱和后继在向量`V`的<u>秩</u>，就可以定位到直接前驱和后继的节点了。

```c++
template <typename T>
struct StaticListNode { // 列表中的一个节点
    T value;            // 本节点存放的数据
    Rank pred;          // 直接前驱的秩
    Rank succ;          // 直接后继的秩
};
```

生成空列表时，需要初始化`2`个哨兵节点。

```c++
template <typename T>
StaticList<T>::StaticList() {
    _size = 0;
    V.push_back(StaticListNode<T>());
    V.push_back(StaticListNode<T>());
    V[_head].succ = _tail; V[_head].pred = -1;
    V[_tail].pred = _head; V[_tail].succ = -1;
}
```

### 在静态链表上删除节点

作为例子，下面展示静态链表删除节点的操作。最自然的想法是模仿动态链表的删除：**算法2.14A**设计算法。

```c++
// 问题2.16 - 静态链表删除节点
// 给定：列表L（静态，基于向量V）
// 输入：待删除的元素的在向量V中的秩r
// 要求：将V[r]从列表L中删除

// 算法2.16A
template <typename T>
void StaticList<T>::remove(Rank r) {
    V[V[r].pred].succ = V[r].succ;
    V[V[r].succ].pred = V[r].pred; 
    --_size;
}
```

在**算法2.16A**中存在一个问题：被删除的节点的内存无法被释放。

> 因为被删除的节点在向量`V`中，所以不能直接用`delete`释放内存。

所以，需要将被删除的节点从向量`V`中删除，以释放`V`中的空间，使其可以被分配给其他节点。

注意，直接删除也是不行的。因为删除`V[0:n]`中的一个节点`V[r]`，会导致`V[r+1:n]`整体前移。`V[r+1:n]`的前移会导致这些节点的秩发生变化，如果其他节点的`pred`/`succ`指向了它们，则需要更新这些指针。所以，如果用从`V`中删除节点的方式来清理内存，则删除一个节点的时间复杂度高达$\Theta(n-r)=O(n)$，这是列表所不能允许的。

为了将删除节点的影响降到最低，则希望`r`越大越好。当`r=n-1`（被删除的节点在向量末尾）时，$\Theta(n-r)=O(1)$，这是列表所理想的结果。

因为列表的元素次序和`V`中的元素次序无关，所以您可以很自然地想到，只需要将`V[r]`交换到`V[n-1]`，就可以顺利在$O(1)$的时间内删除了。

```c++
// 算法2.16B
template <typename T>
void StaticList<T>::remove(Rank r) {
    V[V[r].pred].succ = V[r].succ;     // 先执行列表中的删除
    V[V[r].succ].pred = V[r].pred; 
    --_size;
    Rank last = V.size() -1;           // 再执行向量中的删除
    V[r] = V[last];                    // 将V[last]移动到V[r]的位置上
    V[V[last].pred] = r;               // 连接V[last]的链子转为连到V[r]上
    V[V[last].succ] = r;
    V.pop_back();                      // 删除V的最后一个节点
}
```

对于元素次序不重要的向量结构，在执行删除时，将被删除的元素移动到向量末尾再删除，可以将删除操作的时间复杂度从$\Theta(n-r)$降低为$O(1)$，这是一个常用的技巧。在后面的章节中还会再次出现。

## 分块表

上面的几节介绍了**顺序结构**和**链式结构**两种存储线性表的方式。顺序结构的向量适合循秩访问，而链式结构的列表适合循位置插入和删除。二者在重要操作上的时间复杂度对比如下。

1. <u>循位置插入（删除）元素</u>：链式结构$O(1)$，顺序结构$O(n)$。
2. <u>循秩访问元素</u>：链式结构$O(n)$，顺序结构$O(1)$。

对于循秩的插入（删除），相当于循秩访问再循位置插入（删除），因此时间复杂度是上述两种操作的和。无论是向量还是列表，<u>循秩的插入或删除</u>复杂度都是$O(n)$。

现实中用到的线性表，可能既需要在任意位置插入或删除元素，又需要对给定的秩随机访问。在这种情况下，无论使用顺序结构还是链式结构，都会面临对于其中一些操作效率很低的问题。因此会希望构造一个折中方案。

> 分块表这个章节在清华大学《数据结构》本科教材中是没有的，但不排除被拿来出题；另一些教材会简要介绍这部分内容。但总而言之，这个章节在考研中不是那么重要。因此，笔者只对它做简单介绍，读者也可以选择略读。

### 分块结构

这一节介绍的**分块结构**是一个经典的折中方案，它通过对<u>顺序结构和链式结构的复合</u>，实现了一定程度的效率折中。

> 在一些教材中，“分块”这一节被放在了相当后的位置（《查找》章）。这里将它提前到《线性表》这一章，因为它和前面介绍的线性表关系非常紧密。

分块表的设计思路是“<u>为链表添加**索引**</u>”。

假设一组数据用双链表存储，那么访问靠近头节点和尾节点的数据，是比访问中间某个位置的数据快的。如果已知中间某一段的数据也经常被访问，那么最好的方法就是在这一段中设置一个**索引指针**，当访问到这一段的数据时，不从头节点或尾节点开始遍历，而是<u>从索引指针指向的位置开始</u>。

推而广之，如果中间有两段数据经常被访问，就可以设置两个索引指针。一般地，可以<u>设置一个索引指针组成的线性表</u>。在给定秩随机访问的时候，首先找到最近的索引指针，接着从这个索引指针指向的位置出发遍历到目标位置。这样的操作相当于将原来的<u>“一级遍历查找”变成了“二级遍历查找”</u>。

因此，为了实现两级查找综合效率的最大化（您可以自己尝试用数学方法证明），最好的办法就是取$\sqrt n$个索引指针，每个索引指针负责一个长度为$\sqrt n$的表。这就是建立分块结构的**平方根规则**。

> 从上述分析导出的分块表，它的一级表（索引指针表）可以是向量也可以是列表，二级表（数据表）则是列表。事实上，二级表也可以是向量。这样就形成了`4`种排列组合。这些排列组合并不都具有实际应用的价值，但都有可能出现在试卷上。

### 列表套列表

作为例子，这里只介绍比较常用的“列表套列表”形式的分块表。其他`3`种形式的分块表您可以在阅读完本节之后自己讨论一下。在考研初试中基本不会涉及分块结构的具体实现，只需要了解一种分块表就可以了。

```c++
template <typename T>
struct IndexNode {  // 索引指针节点
    ListNode<T>* p; // 指向的二级表节点
    Rank r;         // 指向的二级表节点在整个表中的秩
};

template <typename T>
class ListList {
private:
    List<IndexNode<T>> L1; // 一级表（索引表）
    List<T> L2;            // 二级表
};
```

作为分块表的整体，它包括一个一级表（**索引表**）、一个二级表（**数据表**）。其中，索引表的每个节点需要记录索引指针所指向的二级表节点，以及它指向的节点在表中的秩。

因为分块表的数据区也是一个列表，所以它可以直接用列表创建：直接让`L2`复制原有的列表，然后建立一级表即可。

```c++
template <typename T>
ListList<T>::ListList(const List<T>& L) {
    L2 = L;
    buildL1();
}
```

其中，`buildL1`是建立索引表的过程。建立索引表只需要在遍历`L2`的过程中，每隔$\sqrt n$个数据节点插入一个索引节点即可。

```c++
// 问题2.17 - 建立索引表
// 给定：数据表L2
// 要求：按照平方根规则生成L1

// 算法2.17A
template <typename T>
void ListList<T>::buildL1() {
    int len = (int)floor(sqrt(L2.size()));     // 两个索引指针之间的间隔，同时也是索引指针的数量
    if (len <= 0) { return; }                  // 空表，直接返回
    int r = -1;                                // 记录数据节点的秩
    L2.traverse([&, len](ListNode<T>* p) -> {
        if (++r % len == 0) {                  // 每隔len个数据节点，插入一个索引节点
            IndexNode<T> temp;
            temp.p = p; temp.r = r;
            L1.push_back(temp);
        }
    });
}
```

### 循秩访问

最能发挥分块表长处的地方是**循秩访问**。

```c++
// 问题2.18 - 列表套列表的循秩访问
// 给定：列表套列表(L1, L2)
// 输入：待访问元素的秩r
// 输出：元素L2[r]的位置
```

在循秩访问的过程中，首先在索引表`L1`中查找，找到`L2[r]`在哪两个索引指向的节点之间。然后在这两个索引截断`L2`形成的长度为$\sqrt n$的列表中寻找即可。

```c++
// 算法2.18A
template <typename T>
ListNode<T>* ListList<T>::operator[](Rank index) {
    auto p1 = L1[0], p2 = p1->succ;
    while (p2 != L1.tail() && p2.r <= index) {
        p2 = (p1 = p2)->succ;
    }
    return p1->value.p->forward(index - p1->value.r);
}
```

> 如果要追求最高的效率，那么需要判断`index`在前半段还是后半段，决定是从前向后还是从后向前检索。在**算法2.18A**中简单地都实现成了从前向后，您可以自己将它改写成带判断的形式。

和普通的列表对比：

* 在列表上直接做循秩访问，时间复杂度是$\Theta(\min(r,n-r))=O(n)$，平均$\Theta(n)$。
* 在分块表上做循秩访问（**算法2.18A**），如果分块表是严格按照平方根规则建立的，那么在一级表上查找的时间为$\Theta(\frac r{\lfloor\sqrt n\rfloor})=O(\frac n{\lfloor\sqrt n\rfloor})=O(\sqrt{n})$，在二级表上查找的时间为$\Theta(r\%\lfloor\sqrt n\rfloor)=O(\sqrt n)$，因此总体的时间复杂度是$O(\sqrt n)$，平均$\Theta(\sqrt n)$。

如果加上从前向后和从后向前的判断，可以降低大约`1/2`的常数。

### 循秩插入和删除

在分块表上做插入、删除和查找的操作，本质上都是在数据表`L2`上做插入、删除和查找的操作。其中，查找是完全一样的；但插入、删除操作则有所不同，因为它会改变后续元素的秩，从而让索引节点里记录的秩必须改变。

如果使用之前设计的`ListList`框架，就会发现，循秩的插入（删除）只需要$\Theta(\sqrt n)$的时间：因为在循秩访问的过程中，已经可以确定被插入（删除）的节点在哪两个索引之间。所以循秩的插入（删除）只需要三步：循秩访问—循位置插入（删除）—更新索引节点。

> 循位置的插入（删除）则需要平均$\Theta(n)$的时间。因为对于循位置的插入（删除），无法直接定位被插入（删除）的位置在哪两个索引节点指向的位置之间。所以，分块表应当避免使用循位置的插入（删除）。

下面展示循秩插入的例子，您可以自己完成循秩删除。

```c++
// 问题2.19 - 列表套列表的循秩插入
// 给定：列表套列表(L1, L2)
// 输入：待插入元素e，目标秩r
// 要求：将e插入到L2[r]的位置上

// 算法2.19A
template <typename T>
void ListList<T>::insert(T e, Rank r) {
    auto p1 = L1[0], p2 = p1->succ;             // 通过循秩访问先找到位置
    while (p2 != L1.tail() && p2.r <= r) {
        p2 = (p1 = p2)->succ;
    }
    auto p = p1->value.p->forward(r - p1->value.r); // 当前的L[r]（可能是tail）
    L2.insertAsPred(p, e);                      // 将e插入到L[r]的位置上
    while (p2 != L1.tail()) {                   // 被插入位置之后的索引，需要将秩+1
        ++p2->value.r;
        p2 = p2->succ;
    }
}
```

> 上面的实现在空表插入第一个节点的时候会发生错误（`L1[0]`没有定义）。请您对**算法2.19A**进行修改，以规避该错误。

> 一个简单的思路是：直接删掉指向它的那个索引。当然，这会导致有两个索引之间的距离变成了$2\sqrt n$；如果被删掉的索引太多，则分块表和平方根规则的距离也会过大，使其无法保证循秩操作在$O(\sqrt n)$时间内完成。
>
> 要维护分块表和平方根规则的距离不至于过大，需要时不时重构索引表，这在下一小节会讨论。
>
> 当然您也可以选择其他思路。比如：将索引的位置向前、或向后、或随机向前向后、或移动到前一个索引和后一个索引的中点处。
>
> 在本节的后续内容中，将使用“索引向后`1`个节点”的策略（如果导致两个索引重合，就删掉一个）。这一策略能保证相邻索引节点指向的秩的距离不会发生突变。

### 重构索引表

在分块表中，一级表（索引表）需要满足两个基本性质：

* 相邻索引节点指向的秩，其距离是$O(\sqrt n)$。
* 索引表的长度是$O(\sqrt n)$。

这两个性质共同保证了分块表的循秩操作都能在$O(\sqrt n)$时间内完成。

> 可理解为“不偏离平方根规则太多”。

为了维护这两个基本性质，需要时不时对索引表进行重构。

```c++
// 问题2.20 - 重构索引表
// 给定：列表套列表(L1, L2)
// 要求：对索引表L1进行重构，以维持平方根规则近似成立
```

设$k_1,k_2>1$是两个先验的比例阈值（为避免重构过于频繁，建议$k_1,k_2\ge 2$），当相邻索引节点指向的秩距离超过$k_1\sqrt n$的时候，对索引表进行**局部重构**（只需要处理这对相邻索引节点的问题）；当索引表的长度超过$k_2\sqrt n$的时候，对索引表进行**整体重构**（需要压缩整体的索引表）。

**局部重构**：设`p1`和`p2`之间的距离超过了阈值。那么，需要对`p1`和`p2`指向的节点之间的二级表进行遍历，每隔$\sqrt n$，插入一个指向它的索引节点。

```c++
// 算法2.20A
template <typename T>
void ListList<T>::localReconstruct(ListNode<IndexNode<T>>* p1) {
    int sqrt_n = (int)floor(sqrt(L2.size())); 
    if (p1->succ->value.r - p1->value.r > k1*sqrt_n) { // 判断是否需要重构
        auto p = p1->value.p;            // p1指向的节点
        for (int i = 1; i < k1; ++i) {   // 在中间插入k1-1个索引
            p = p->forward(sqrt_n);      // p移动到下一个被插入的索引指向的位置
            L1.insertAsSucc(p1, IndexNode<T>());      // 插入新的索引节点
            p1->succ->value.r = p1->value.r + sqrt_n;
            p1->succ->value.p = p;
            p1 = p1->succ;
        }
    }
}
```

局部重构的时间复杂度是$\Theta(k_1\sqrt n)$。同时，因为引发局部重构本身需要两个节点之间的距离超出阈值，而每次插入操作只会让这个距离增加`1`，所以要$\Theta(k_1n)$次插入操作，才会引发`1`次局部重构。因此，局部重构的分摊时间复杂度仅为$O(1)$。

**整体重构**：当`L1`的长度超出阈值时，反过来，说明相邻索引节点之间的距离太小了。此时就需要删除掉一些索引节点，来保持索引节点之间的距离在合理的区间内，从而保证`L1`的长度不超过阈值。

> 如何选择索引节点进行删除，是一个比较开放的问题。
>
> 下面的示例代码使用的策略是：设`p1,p2,p3`是连续的三个索引节点，如果`p1`和`p3`指向的节点的秩的距离不超过$\sqrt n$，那么就将`p2`删除。在这种策略下，可以证明重构之后`L1`的长度不超过$2\sqrt n$。这也就意味着必须取$k_2>2$。
>
> 您也可以自己设计删除索引节点的策略。

```c++
// 算法2.20B
template <typename T>
void ListList<T>::globalReconstruct() {
    int sqrt_n = (int)floor(sqrt(L2.size()));
    if (L1.size() > k2*sqrt_n) {                  // 判断是否需要重构
        auto p = L1[0];
        while (p->succ->succ != L1.tail()) {      // 逐个判断，是否需要删除p->succ
            if (p->succ->succ->value.r - p1->value.r <= sqrt_n) {
                L1.remove(p->succ);               // 满足条件，删除p->succ
            } else {                              // 否则继续向后检索
                p = p->succ;
            }
        }
    }
}
```

整体重构的时间复杂度是$\Theta(k_2\sqrt n)$，和局部重构一样，分摊的时间复杂度也只有$O(1)$。

综上所述，上述两种索引表的重构，可以保证在分块表规模不断扩大（或缩小）的时候，可以以分摊$O(1)$的时间对索引表进行重构，从而维护索引表的两个基本性质，保证循秩操作能在$O(\sqrt n)$时间内完成。

