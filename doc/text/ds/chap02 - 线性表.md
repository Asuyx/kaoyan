# 线性表

**线性表**是指相同类型的有限个数据组成的序列。在这本笔记中采用的是清华大学教材的分法，将线性表包括**向量**（vector）和**列表**（list）两种形式，分别对应C++ STL里的`vector`和`list`。

> 在另一些教材中，这两个词被称为**顺序表**和**链表**，分别对应Java里的`ArrayList`和`LinkedList`。向量（顺序表）和列表（链表）这两对概念通常可以混用。

向量和列表代表着两种最基本的数据结构组织形式：**顺序结构**和**链式结构**。本章在分别介绍这两种数据结构之后，将会进一步介绍这两种结构级联得到的**分块结构**。

## 向量

> 中国革命必须分为两个步骤。第一步，改变这个殖民地、半殖民地、半封建的社会形态，使之变成一个独立的民主主义的社会。第二步，使革命向前发展，建立一个社会主义的社会。
>
> ——Chairman Mao

**向量**（vector）是一个基于**数组**（array）的数据结构，因此向量在内存中占据的是一段连续的空间。

### 向量和数组

**元素**（element）代表数据结构中的单个数据单元，它可能是基本数据类型、结构体、函数等。一个数据结构中的元素具有统一的类型。所有数据结构都是由元素构成的。

**线性表**是元素成线性排列的表。作为一种基于数组的线性表，<u>向量的元素次序和数组的元素次序相同</u>。如果一个向量`V`基于数组`A[0:m]`构建，那么向量`V`的第`i`个元素就是`A[i]`。

> 说向量元素的**物理次序**和**逻辑次序**相同是不妥的，尽管这种说法可能能帮助您理解向量，但可能会引起新的混乱。
>
> 在《计算机组成原理》和《操作系统》学科中您将会看到，<u>“物理”和“逻辑”这两个相对的名词一般用于**实际内存**和**虚拟内存**的场合</u>。数组在虚拟内存上的逻辑地址总是连续的，但在实际内存上的物理地址受操作系统调度影响，可能由多个不连续的部分组成。所以用“物理次序”这样一个有歧义的名词是不妥的。

需要注意的是，尽管向量和它基于的数组在<u>元素次序上相同</u>，但在<u>元素数量上是不一定相同</u>的。数组的元素数量总是恒定的，而向量的元素数量是<u>运行时可变的</u>。

记一个长度为`n`的向量为`V[0:n] = { V[0], V[1], ..., V[n-1] }`，则`n`称为向量`V`的**规模**（size），而称为向量`V`分配的内存空间可以容纳的元素数量`m`（即它所基于的数组的规模）为向量`V`的**容量**（capacity）。

> 一个向量的<u>规模必定不大于容量</u>。在不超过容量的前提下，向量的规模可以灵活变化，从而赋予了它比数组更高的灵活性。

对于向量中的每一个元素`V[i]`来说，它前面的元素称为它的**前驱**（predecessor），它后面的元素称为它的**后继**（successor），特别地，和它位置相邻的前驱，也就是**直接前驱**为`V[i-1]`，相应地，**直接后继**为`V[i+1]`。所有的前驱构成了**前缀**（prefix），也就是`V[0:i]`；所有的后继构成了**后缀**（suffix），也就是`V[i+1:n]`。

### 循秩访问

在本笔记中介绍的各种数据结构，将从一个非常小的基础模板开始逐步添加功能。您可以跟随笔记的正文自己实现模板，或参考配套代码中的模板。

> 为了让模板更有用，配套代码里还实现了一些和《数据结构》知识关系不大的函数。这些内容在笔记正文中不会介绍。

在上一小节中已经介绍过向量的基本元素：用于存放数据的数组，以及规模、容量两个基本属性。据此，可以写出一个基本的框架如下。

```c++
template <typename T>
class Vector {
    T* _data;      // 向量所基于的数组
    int _capacity; // 向量的容量
    int _size;     // 向量的规模
public:
    int capacity() { return _capacity; }
    int size() { return _size; }
};
```

> 这里将规模和容量都定义成了**私有**（private）变量，并提供了公共的`getter`方法。因为向量外的代码不应该直接修改它们。

对于向量中的元素，其访问方式称为**循秩访问**。称元素`V[i]`在向量`V`中的序号，也就是`i`，为它的**秩**（rank）。对于建立在数组`A`上的向量`V`，因为`V`和`A`的元素次序是一致的，所以`V[i] = A[i] = *(A+i)`。因此，只要知道一个元素的秩，就可以在$O(1)$的时间内访问该元素。

```c++
typedef int Rank;

template <typename T>
T& Vector<T>::operator[](Rank index) {
    return _data[index];
}
```

> 如果`index`超出了`capacity`，会发生数组越界，可能引起**段错误**（segmentation fault）。如果追求稳健性，应当增加一个`index < capacity`的判断。但为效率起见，这里舍弃了这一判断。
>
> 后文中同样会舍弃一些稳健性的考量。

### 装填因子

设向量的容量为`m`，规模为`n`，则称比值`n/m`为**装填因子**（load factor）。正常情况下，这是一个`[0,1]`之间的数。装填因子是衡量向量效率的重要指标。

<u>如果装填因子过小</u>，则会造成内存浪费：申请了巨大的数组，但其中只有少量的单元被向量中的元素用到，其他单元都被闲置了。

<u>如果装填因子过大</u>（超过1），则会引发数组越界，造成段错误。

> 刚开始的时候，装填因子一定是在`[0,1]`之间的。
>
> 但因为数组的容量`m`是固定的，而向量的规模`n`是动态的，所以一开始分配的`m`可能后来会不够用，从而产生装填因子大于1的问题。

为了让装填因子保持在一个合理的范围，需要允许动态地改变容量`m`的值。令m增大的操作称为**扩容**，令m减小的操作称为**缩容**。

### 改变向量的容量

首先来看向量扩容。

```c++
// 问题2.1 - 向量扩容
// 分析合适的方式让向量扩容
```

这就不是上一章那种简单的算法问题了，需要逐步分析。

首先，直接在原向量所占空间后面增加“一条尾巴”是不现实的，因为原向量所占空间后面的地址，可能已经被分配给了其他变量。因此，合适的做法是<u>重新建立一个更大的数组，将原向量的数据复制到新数组中，再把原数组的空间释放掉</u>。

> 注意这个操作对于缩容也是一样的，只是缩容是建立了一个更小的数组。

```c++
// 算法2.1A
template <typename T>
void Vector<T>::set_capacity(int new_capacity) {
    T* old = _data;
    _data = new T[_capacity = new_capacity];
    arrayCopy(_data, old, _size);
    delete[] old;
}
```

> 这里`arrayCopy(T* dst, T* src, int size)`函数用来将大小为`size`的数组从`src`移动到`dst`位置。

设`new_capacity`为`M`，则上述**算法2.1A**的时间和空间复杂度均为$O(M)$。

### 等差扩容和等比扩容

**算法2.1A**仍然没有解决关键问题：在实际使用中，用户不一定知道应该扩容到多少：如果新的容量`M`比较小，就可能再次发生数组越界，就需要再次调用`set_capacity`函数扩容，<u>消耗时间</u>；如果`M`比较大，则装填因子太低，<u>浪费空间</u>。

<u>所以，向量的设计者应该提供一个合理的扩容规则</u>。如果用户知道怎么扩容当然最好，但当用户不知道应该扩容到多少的时候，这个<u>合理的扩容规则</u>给了他们一个备选项。只要选择按照向量设计者提供的规则来扩容，使得时间和空间效率都不会太低。

那么应该如何设计扩容规则呢？

第一种方法是<u>按照等差数列扩容</u>，规定一个公差`d`，每次扩容都让容量`+d`（**固定扩容**），也就是从`m`变成`m+d`；第二种方法是<u>按照等比数列扩容</u>，规定一个公比`q`，每次扩容都让容量`*q`（**比例扩容**），也就是从`m`变成`qm`。

```c++
// 算法2.1B - 基本框架
template <typename T>
void Vector<T>::expand(function<int(int)> strategy) {
    set_capacity(strategy(_capacity));
}

// 算法2.1B1 - 固定扩容
template<typename T>
void expandByAP(const Vector<T>& V, int d) {
    V.expand([=](int m) -> { return m + d; });
}

// 算法2.1B2 - 比例扩容
template<typename T>
void expandByGP(const Vector<T>& V, double q) {
    V.expand([=](int m) -> { return (int)(m * q); });
}
```

> 这里允许**比例因子**`q`不是整数。实际使用时，如果比例因子是整数，则应当声明`q`为`int`类型以加快运算速度。特别地，如果`q`是2的幂次，可以写成移位计算的形式进一步加快运算速度。

很显然，在`strategy`函数只进行了简单的加（或乘）运算的情况下，`expand`只进行了一次`set_capacity`的操作，其效率和单次的`set_capacity`基本一致。

那么，<u>固定扩容和比例扩容</u>这两种扩容方式哪种更好？它们都只调用了一次`expand`，所以它们的效率是一样的吗？

显然不是的。设计**算法2.1B1**和**算法2.1B2**的原理，是按照等差或等比“<u>*数列*</u>”扩容，而不是“<u>*一次*</u>”扩容。所以评价这两种扩容规则的标准，不是<u>进行一次扩容</u>的效率或<u>进行一次扩容</u>后的装填因子，而是比较<u>一系列扩容</u>操作的总体效率和在这<u>一系列扩容</u>操作中的平均装填因子。用已有的复杂度分析工具不足以对这两种策略的效率进行准确评价。

### 分摊复杂度分析

为了对**一系列操作**进行分析，需要引入新的复杂度分析标准。

一般地，假设$O_1, O_2, \dots, O_n$是连续进行的`n`次操作，则当$n\to\infty$，这`n`次<u>连续操作所用时间的平均值的复杂度</u>，称为这一操作的**分摊复杂度**，对分摊复杂度的分析称为**分摊分析**。分摊分析的原则之一是：<u>使用相同效果的操作序列</u>。所以，要比较**算法2.1B1**和**算法2.1B2**，不应该把每次操作取为"进行一次扩容"（因为两种方法扩容量不一样)，而应该取为“向量`V`的规模增加`1`”。连续进行`n`次操作，就可以考虑向量`V`的规模从`0`增长为`n`的过程。

**算法2.1B1**中，容量依次被扩充为$d,2d,3d,\dots,n$，共进行$n/d$次扩容。

因此，分摊复杂度为：
$$
T(n) = \frac{d + 2d + 3d + \dots + n}n = \frac{(n/d)\cdot d + (n/d)(n/d-1)/2\cdot d}n = \frac{n/d+1}2 = O\left(\frac nd\right)
$$
另一方面，进行k次扩容之后的装填因子至少为$\frac{kd}{(k+1)d}=\frac{k}{k+1}$，当$k\to\infty$时，装填因子趋于**100%**。

**算法2.1B2**中，容量被依次扩充为$q,q^2,q^3,\dots,n$，共进行$\log _q n$次扩容。

因此，分摊复杂度为：
$$
T(n)=\frac{q+q^2+q^3+\dots+n}n=\frac{q\cdot\frac{1-n}{1-q}}n=O(1)
$$
另一方面，装填因子不断在$[\frac 1q,1]$之间线性增长，平均装填因子为$\frac{1+q}{2q}$。可以看出，不管怎样选择`q`，对分摊复杂度都没有影响，而更小的`q`能够带来更大的平均装填因子。因此，在实际的向量扩容过程中，总是选择`q = 2`，这时平均装填因子为**75%**。

从上面的分摊分析中可以看出，**算法2.1B1**尽管能够带来更高的装填因子，但时间效率略有不足；而**算法2.1B2**则在保证装填因子不太低的情况下，时间效率非常优秀。

所以，通常向量扩容会选择**算法2.1B2**的规则。

> 不过，**算法2.1B2**也有其劣势，就是容量越大，装填因子不高带来的空间浪费愈发明显，所以有些对空间要求较高的情况下，也采用二者相结合的方式：<u>在容量比较小时加倍扩容、在容量比较大的时候固定扩容</u>，在《计算机网络原理》的学习中您将看到二者相结合的例子。

### 缩容

介绍完扩容，缩容的方法也呼之欲出了：无非是**固定缩容**或者**减半缩容**。您可以自己完成这两个算法。

缩容的重要性远不如扩容，因为缩容降低`m`之后，如果`n`又扩大回去了，就又要扩容回去，这一缩一扩浪费了不少时间，而发挥的价值甚微（除非在这段缩、扩之间的时间里，释放出的内存另有他用）。为了避免一缩一扩浪费时间，通常会规定一个**缩容阈值**，当装填因子小于这个阈值的时候才会缩容。

对于常用的<u>比例扩容+比例缩容</u>的策略，您可以自己证明，只要<u>缩容阈值<50%</u>，就可以保证对于任意的操作序列（每个操作是`n`加一或减一），扩容和缩容的<u>综合分摊复杂度</u>为$O(1)$。在实际的向量中，为了方便计算，缩容阈值通常使用25%、12.5%等数值，取为0（**禁止缩容**）也是一个常见的策略。

### 插入单个元素

对于任何数据结构，都有三种基本的操作：

* **插入**（insert）：向数据结构中插入一个元素。

* **查找**（find）：查找一个元素在数据结构中的位置。

* **删除**（delete）：从数据结构中移除一个元素。

  > 因为`delete`是许多语言的关键字，所以在编程的时候常用`remove`代替它。

下面就以向量为例，分别介绍这三种基本的操作。

```c++
// 问题2.2 - 向量插入元素
// 给定：向量V[0:n]
// 输入：待插入的元素e，目标的秩r
// 要求：将元素e插入到V[r]的位置上
```

首先讨论插入。

要将待插入的元素`e`插入到`V[r]`，那么可以将原来的向量`V[0:n]`分成`V[0:r]`和`V[r:n]`两部分。

* 插入之前，向量是`V[0:r]`，`V[r:n]`。
* 插入之后，向量是`V[0:r]`，`e`，`V[r:n]`。

可以发现，在插入的前后，前一段`V[0:r]`的位置是不变的，而后一段`V[r:n]`需要整体向后移动1个单元的位置。据此，可以设计下面的算法。

```c++
// 算法2.2A
template <typename T>
void Vector<T>::insert(T e, Rank r) {
    if (_size + 1 > _capacity) { expand(); }    // 插入后会超出容量，需要扩容，此处省略扩容策略
    arrayCopy(_data+r+1, _data+r, _size-r, -1); // 从后向前，依次移动V[r:n]中的每个元素
    _data[r] = e;
    ++_size; // 更新向量的规模
}
```

> `arrayCopy`那行语句从功能上等价于`V[r+1:n+1] = V[r:n]`，就是将`V[r:n]`整体后移1个单元。
>
> 这里`arrayCopy`的最后一个参数表示从后向前更新（您可以自己想一下，如果从前向后更新会发生什么？）。`arrayCopy`的实现见配套代码。

不考虑扩容（前面已经证明，扩容的分摊复杂度是$O(1)$，它不会影响到其他操作的复杂度），则单次插入的时间复杂度为$\Theta(n-r)$，空间复杂度$O(1)$。

### 平均复杂度分析

为了更定量地分析插入操作的时间效率，引入一个新的复杂度分析策略：**平均复杂度**。

在介绍复杂度时曾经强调，复杂度是依赖于<u>数据规模</u>，不依赖于<u>具体情况</u>的分析手段。在**算法2.2A**中，数据规模通常认为是`n`，而`r`是具体情况带来的参数。为了研究不同具体情况对算法时间效率的影响，有三种常见的分析手段：

* **最坏时间复杂度**：研究在情况最坏的情况下的复杂度。

  很多算法有硬性的时间限制（如<u>在复试的机试中，通常要求输出结果的时间不能多于1s或2s</u>），此时常常使用最坏时间复杂度分析。这是最常用的时间复杂度分析。

* **最好时间复杂度**：研究在情况最好的情况下的复杂度。

  研究最好时间复杂度的意义远小于最坏时间复杂度。最好时间复杂度往往用于嘲讽某种算法的效率：在最好的情况下，这种算法的复杂度也只能达到XXXX，而我的新算法可以达到XXXX。

* **平均时间复杂度**：研究在平均情况下的复杂度。

  如果没有硬性的时间限制，则平均时间复杂度往往能更好地反映一个算法的总体时间效率。

  平均时间复杂度需要知道<u>各种情况的**先验**概率</u>，在这个概率的基础上计算$T(n)$的**数学期望**的复杂度。在针对显示数据的实验研究中，常见的假设包括正态分布、gamma分布和Poisson分布；而在《数据结构》学科中，通常假设成<u>等可能的分布</u>，以方便进行理论计算。

  > 分摊复杂度是一系列连续操作的平均效率，而平均复杂度是单次操作的期望效率。
  >
  > 分摊复杂度的一系列连续操作是有可能存在后效的，而平均复杂度只讨论单次操作的可能情况。
  >
  > 分摊复杂度需要指定每次进行何种的**基本操作**，而平均复杂度需要指定各种情况的**先验概率**。
  >
  > 这两个概念务必加以区分。

> 最坏、最好、平均时间复杂度对应统计里的**最大值**、**最小值**和**数学期望**。显然，其他统计量，比如**方差**、**标准差**，在分析的时候也是有价值的，也深得科研人员重视。但在《数据结构》的考试中，是不会涉及到这些统计量的分析的，只需要知道最坏、最好和平均时间复杂度的分析技术即可。

现在回到插入的**算法2.2A**，它的时间复杂度是$\Theta(n-r)$。显然，最好时间复杂度是$O(1)$（插入在末尾的情况），最坏时间复杂度是$\Theta(n)$（插入在开头的情况）。

为了求平均时间复杂度，一个合理的假设是，`r`的取值对于`[0:n]`之间的整数是等概率的。在这个假设下，容易算出单次插入的<u>平均时间复杂度</u>为$\Theta(n)$。

### 插入批量元素

如果要插入的不是单个元素，而是多个元素，情况会发生什么变化呢？

```c++
// 问题2.3 - 向量插入多个元素（向量合并）
// 给定：向量V[0:n]
// 输入：待插入的向量V1[0:n1]，目标的秩r
// 要求：将向量V1整体插入到V中，其中V1[0]插入到V[r]的位置上
```

在**问题2.3**中，需要插入`n1`个连续的元素。最简单的想法是直接调用`n1`次**算法2.2A**，然而这样的总时间复杂度高达平均$\Theta(n\cdot n_1)$，略显笨重。

您可以敏锐地发现，只要再次使用在讨论单元素插入时的分析方法，就可以得到更加高效的算法。

要将待插入的向量`V1`插入到`V[r]`，那么可以将原来的向量`V[0:n]`分成`V[0:r]`和`V[r:n]`两部分。

* 插入之前，向量是`V[0:r]`，`V[r:n]`。
* 插入之后，向量是`V[0:r]`，`V1`，`V[r:n]`。其中，`V[r:n]`被转移到了`V[r+n1:n+n1]`的位置上。

```c++
// 算法2.3A - 向量插入元素（批量）
template <typename T>
void Vector<T>::insert(const Vector<T>& V, Rank r) {
    int new_size = _size + V._size;                   // 计算插入之后的向量规模
    if (new_size > _capacity) { expand([=](int m) -> int {
        return max(new_size, default_expand_strategy(m));
    }); }        // 插入后会超出容量，需要扩容。这里省略指定的default_expand_strategy
    arrayCopy(_data+r+V._size, _data+r, _size-r, -1); // 从后向前，依次移动V[r:n]中的每个元素
    arrayCopy(_data+r, V._data, V._size);             // 依次插入V1中的元素
    _size = new_size; // 更新向量的规模
}
```

**算法2.3A**的平均时间复杂度为$\Theta(n+n_1)$，比连续调用`n1`次**算法2.2A**要优秀得多。**算法2.3A**中体现出的“<u>用块操作代替多次单元操作</u>”的思想，在以线性表为背景的算法设计题中应用广泛。

> **算法2.3A**仍然有继续改进的空间。
>
> 在**算法2.3A**的实现中，<u>插入操作和扩容操作</u>是解耦的。事实上，在扩容申请了新的数组空间之后，没有必要先把原数组的元素复制过去再移动。移动可以复制到新数组空间的同时进行，从而减少一次`arrayCopy`移动的时间。您可以自己实现这个改进版本的算法。

### 删除单个元素

删除元素是插入元素的逆操作。在插入元素时，让<u>被插入元素的后继后移</u>；因此在删除元素的时候，只需要让<u>被删除元素的后继前移</u>即可。

```c++
// 问题2.4 - 向量删除元素
// 给定：向量V[0:n]
// 输入：待删除元素的秩r
// 要求：将V[r]删除

// 算法2.4A - 向量删除元素（单元）
template<typename T>
void Vector<T>::remove(Rank r) {
    --_size;  // 更新向量的规模
    arrayCopy(_data+r, _data+r+1, _size-r); // 这里可以从前向后依次前移
    shrink(); // 如果有必要，则缩容
}
```

删除操作同样是时间复杂度$\Theta(n-r)$，平均时间复杂度$\Theta(n)$，空间复杂度$O(1)$。

对于批量删除操作（一次删除`V[r1:r2]`的所有元素），可以用与批量插入相似的方法解决，请您自己设计相关算法。

> 这里讨论的向量的“删除元素”严格地说是“循秩删除”。
>
> 另一种删除的方式是，“删除满足某一条件的所有元素”，这个问题将在后面的小节里讨论。

### 查找单个元素

在向量查找一个元素，只需要得到被查找元素的**秩**就可以了，因为`V[i]`的地址就是其所基于的数组的首地址偏移`i`个。和插入、删除相比，查找具有更加丰富的灵活性，甚至于一些编程语言（如SQL）的核心就是查找。

最简单的查找是**按值查找**。即，给定被查找元素的值，在数据结构中找到等于这个值的元素。

```c++
// 问题2.5 - 向量查找
// 给定：向量V[0:n]
// 输入：待查找的元素e
// 输出：元素e在向量V中的秩
```

对于这个问题，最简单的方案就是<u>检测向量中的每个元素</u>是否等于`e`，如果等于，就把它的秩返回。

```c++
// 算法2.5A
template<typename T>
Rank Vector<T>::find(T e) {
    for (Rank i = 0; i < _size; ++i) { // 检测每个元素是否等于e
        if (_data[i] == e) {
            return i;                 // 如果相等则返回秩
        }
    }
    return -1;                        // e不在向量中，返回-1
}
```

设`e`在向量中的秩为`r`，那么在<u>查找成功的情况</u>下，上述算法的时间复杂度为$\Theta(r)$。在<u>查找失败的情况</u>下，算法的时间复杂度为$\Theta(n)$。这里可以分析，在等可能条件下，查找成功时的平均时间复杂度是$\Theta(n)$。

> 查找成功的概率是一个很难假设的值，所以在分析平均时间复杂度时，通常只分析“查找成功时”和“查找失败时”的平均时间复杂度，而不会将它们混为一谈。

因为对于向量`V`和待查找元素`e`的情况没有更多的先验信息，所以暂时也没有比**算法2.5A**更高效的解决方案了。

> **利用信息思考**是计算机领域重要的思维方式。在设计算法时，应尽可能利用更多的先验信息。反之，如果先验信息不足，则算法的效率受到信息论限制，不可能会特别高。这个思维方式在后文介绍各种算法的设计过程时，还会反复出现。

但**算法2.5A**还是有一些值得推敲的地方：如果`e`在向量`V`中出现了多次，那么**算法2.5A**只会返回**最小的秩**。您可以思考一下，如何将其修改成返回**最大的秩**的算法？修改后的算法复杂度和**算法2.5A**有区别吗？

### 查找批量元素

进一步地，如何修改成返回**所有的秩**？因为要返回的是所有的秩，所以不能像**算法2.5A**或它的“最大秩修改版”那样，找到一个等于`e`的元素就直接`return`，必须老老实实地判断向量中的每个元素是否等于`e`。

将这个条件更加一般化，既然可以查找所有等于`e`的元素，那么对于任意给定的条件，也就可以查找所有满足该条件的元素。这个条件可以是“等于`e`”，也可以是“大于`e`”，甚至可能不但和元素本身有关，还和它的秩有关。

```c++
// 问题2.6 - 向量查找
// 给定：向量V[0:n]
// 输入：需要满足条件filter
// 输出：所有满足条件的元素在向量V中的秩
```

<u>以某种次序访问数据结构中的每个元素有且仅有一次</u>，这一过程称为**遍历**（traverse）。刚才讨论过，遍历的时候可能不只要用到元素本身，还需要用到它的秩，所以遍历函数`visit`不能只接受一个`T&`类型的元素本身的引用，还需要接受一个表示秩的参数。

```c++
// 算法2.6A
template<typename T>
void Vector<T>::traverse(function<void(Rank, T&)> visit) {
    for (Rank i = 0; i < _size; ++i) {
        visit(i, _data[i]);
    }
}
```

上面的代码是最为经典的向量遍历模式：从`V[0]`遍历到`V[n-1]`，称为**顺序遍历**。如果是反着从`V[n-1]`遍历到`V[0]`，则称为**倒序遍历**。在一些特殊的问题中，还可能会用到其他遍历方法。

> 在本节的后续小节里，会举例介绍这一点。

遍历要求访问数据结构的每个元素各一次。重复访问或者中途退出都不是完整的遍历。因此，如果不考虑访问的次序，且`visit`只会读写自己访问的那个元素，那么不同方式的遍历产生的结果是一样的。在批量查找这个场合就是这样，无论**算法2.6A**的遍历算法是怎么写的，都可以统一地通过下面的**算法2.6B**完成批量查找。

```c++
// 算法2.6B
template <typename T>
Vector<Rank> Vector<T>::findAll(function<bool(Rank, const T&)> filter) {
    Vector<Rank> temp;
    traverse([&temp](Rank index, const T& e) -> void {
        if (filter(index, e)) {
            temp.push_back(index);
        }
    });
    return temp;
}
```

> 其中`push_back`意为在尾部添加元素，即`insert(e, _size)`。

在`filter`的时间复杂度为$O(1)$的情况下，由于`push_back`的均摊复杂度为$O(1)$，所以遍历过程中，访问每个元素时需要的时间为$O(1)$；故上述算法的总体时间复杂度为$\Theta(n)$。

### 删除批量元素

这一小节讨论之前遗留下来的一个问题：如何删除向量中<u>所有满足条件</u>的元素？

> 这个问题和上一小节讨论的“如何*查找*向量中<u>所有满足条件</u>的元素”是非常相似的。

```c++
// 问题2.7 - 向量删除元素
// 给定：向量V[0:n]
// 输入：需要满足条件filter
// 要求：删除向量V中所有满足条件的元素
```

一个最朴素的思想是：在向量`V`中查找满足`filter`条件的元素，然后将它删除；直到没有满足`filter`条件的元素为止。

```c++
// 算法2.7A（逐个查找-逐个删除）
template <typename T>
void Vector<T>::removeAll(function<bool(Rank, const T&)> filter) {
    Rank index;
    while ((index = find(filter)) >= 0) {
        remove(index);
    }
}
```

这里仍然不妨假设`filter`的时间复杂度是$O(1)$，以排除`filter`对删除算法复杂度造成的影响。

上述**算法2.7A**的空间复杂度是$O(1)$，但是时间效率是很低的。设`r=index`，那么根据之前几个小节的实现，`find`的时间复杂度为$\Theta(r)$，而`remove`的时间复杂度为$\Theta(n-r)$，因此每一遍循环，时间复杂度均为$\Theta(n)$。在最坏的情况下，所有的`n`个元素都满足`filter`的条件，所以总的时间复杂度为$\Theta(n)+\Theta(n-1)+\dots+\Theta(1)=\Theta(n+(n-1)+\dots+1)=\Theta(n^2)$。

这个时间复杂度显然不能接受。之前介绍的“批量插入”和“批量查找”，都是线性时间复杂度，对于批量删除，无论如何都无法接受平方级的时间复杂度。

> 在设计算法的时候，题目不一定会给出要求的复杂度。这个时候，可以对比一下<u>“相似问题”的复杂度</u>。
>
> 为了降低时间复杂度，就要设法降低在算法中进行的**不必要工作**。在不必要工作中，最典型的一种是**重复工作**，它代表了在算法中，反复计算了同一算式造成的时间效率浪费。

对于**算法2.7A**而言，它有一项非常明显的重复工作：假设第一遍`find`的返回值是`r`并删掉了`V[r]`，那么在进行第二遍`find`的时候，`V[0:r]`中的元素仍然被检索了一遍，但实际上，第一遍`find`已经检索过它们了。

为了消去这一重复工作，则在第二遍`find`的时候，不应该再从`V[0]`开始检索，而应该从`V[r]`开始检索。

```c++
// 算法2.7B（一次查找-逐个删除）
template <typename T>
void Vector<T>::removeAll(function<bool(Rank, const T&)> filter) {
    for (Rank i = 0; i < _size; ) {
        if (filter(i, _data[i])) {
            remove(i);
        } else {
            ++i; // 注意，如果remove了则不需要++i，否则会跳过1个元素
        }
    }
}
```

> 和这种方法相似的一个方法，是用`findAll`找出所有满足`filter`的元素，然后逐个删除。您可以自己实现它：它会多耗费最坏$\Theta(n)$的空间，而对时间效率没有影响。

然而，在最坏的情况下（所有元素都要被删除），光是`remove`就要花费$\Theta(n^2)$的时间，**算法2.7B**的优化程度仍然不够。

因此，下一步优化就要从`remove`入手，需要将`remove`的工作展开来，看看其中哪些是不必要的。在`remove`中，主要消耗时间的是<u>元素移动</u>的操作。您可以发现，如果`V[0:i]`中有`k`个元素要删除，那么最后一个元素`V[i-1]`就要向前移动`k`次：依次移动到`V[i-2]`、`V[i-3]`、……、`V[i-k-1]`的位置上。看上去这些操作并没有**重复工作**，但它们是另一类典型的不必要工作：**不到位工作**。这一系列的移动被拆成了`k`次，而实际上是可以一步到位，直接从`V[i-1]`移动到目标位置`V[i-k-1]`的。

为什么可以直接移动到目标位置呢？注意到，在**算法2.7B**中，当检索到`V[i]`的时候，`V[0:i]`的所有元素都已经被检索过了，因此`k`的值已经确定了，并且前`i-1`个元素已经移动到了正确的目标位置。所以您可以用归纳法的思路，证明直接移动的正确性。证明完成之后，剩下的就只有编码的工作了。

```c++
// 算法2.7C（一次查找-一次删除）
template <typename T>
void Vector<T>::removeAll(function<bool(Rank, const T&)> filter) {
    int k = 0;      // 用来记录偏移量，即V[0:i]中满足filter的数量
    for (Rank i = 0; i < _size; ++i) {
        if (filter(i, _data[i])) {
            ++k;    // 满足filter条件，记录偏移量
        } else {
            _data[i-k] = _data[i]; // 不满足filter，移动元素
        }
    }
    _size -= k;     // 直接缩减_size抛弃掉末尾的元素
    shrink();       // 如果有必要，则缩容
}
```

非常显然，现在时间复杂度被缩减到$\Theta(n)$了。

> 可以看出，**算法2.7C**中还是有一些不必要工作。在`k=0`的时候，会产生没有意义的赋值操作。但这一数量的不必要操作，不会对算法的时间复杂度产生影响，所以通常优化到这个层次就可以了。您可以自己尝试将没有意义的赋值操作去掉。

**算法2.7C**的思路可以被概括为**快慢指针**。快指针即探查指针，指向`V[i]`；慢指针即更新指针，指向`V[i-k]`。快指针找到需要保留的元素，然后将它们移动到慢指针的位置处。

### 随机置乱

一般数据结构重点讨论的只有**插入**、**删除**和**查找**三种基本操作，但向量作为一种非常基础的数据结构，经常被用来在考试中作为算法设计题的背景。下面这两个小节分别从熵增和熵减的角度出发，讨论**置乱**和**排序**的算法。

这一小节先讨论置乱。

```c++
// 问题2.8 - 向量置乱
// 给定：向量V[0:n]
//      在这个问题中，假定rand()能随机产生一个正整数
// 要求：随机打乱向量V中的元素
```

> 现实中的`rand`是**伪随机**。对于同一个种子，生成的伪随机序列是相同的；所以并不能真正“随机”地打乱向量中的元素。伪随机问题不是《数据结构》研究的要点也不会考到；关于伪随机的一些讨论见《算法设计》部分。

直接看这个“向量置乱”的问题，很容易没有头绪。不妨将这个问题迁移到比较熟悉的领域：比如洗牌。

想必大家都非常熟悉洗牌。随机置乱的目的和洗牌是一样的，但如果用洗牌的方法去做随机置乱，即抽出一沓牌、把这沓牌放到牌堆底部、再抽一沓牌，则会面临三个问题：

1. 您不知道重复多少次抽牌比较合理；
2. 在有限次抽牌之后，牌的`n!`种随机次序并不是等概率的；
3. 每次抽牌都要伴随大量的元素移动，时间效率非常低下。

解决随机置乱问题可以从上面的第二个问题，也就是“<u>随机次序等概率</u>”入手。

为了保证随机次序是等概率的，那么就要构造`n!`种等可能的情况。根据乘法原理，可以很自然地想到，如果将每种次序表示为一个`n`元随机变量组$(X_1, X_2, ..., X_n)$，其中$X_i$两两独立，并且$X_i$恰好有`i`个等可能的取值，那么这`n!`种次序就是等可能的了。接下来，只需要建立在全排列和这样的`n`元组的一一对应的映射关系即可。

> 当然不能直接把全排列用上。全排列的两个元素不是相互独立的，它自身不是符合条件的`n`元组。

为了构造符合条件的映射，又可以采用递归的思想方法：

* 如果`n = 1`，全排列和`n`元组可以直接对应。
* 对于`n > 1`，考虑`V[n-1]`在打乱后的秩，显然，它可以取`0`、`1`、……、`n-1`这`n`个等可能的值，令这个数为$X_n$，然后将`V[n-1]`从打乱前后的向量中都删除，就化为了`n-1`的情况。反复利用这个化归方法，最终可化归到`n = 1`的情况。

以上就成功构造出了满足条件的一一映射关系，您可以在理解它的基础上自己设计相应的随机置乱算法。

```c++
// 算法2.8A
template <typename T>
void Vector<T>::shuffle() {
    for (Rank i = _size; i > 0; --i) {
        swap(_data[i-1], _data[rand() % i]);
    }
}
```

显然上面这个算法是时间$\Theta(n)$、空间$O(1)$的。并且上面的分析表明，如果`rand()`真的能随机生成一个非负整数（不是随机生成一个非负`int`！），那么**算法2.8**就能将所有的`n!`个排列等概率地输出。

> `rand()`如果随机生成一个非负`int`，那么`n`次`rand()`一共只有$2^{32n}=o(n!)$种可能的取值，所以在`n`充分大的时候，必然会有一些排列不可能被输出。
>
> 并且，不管这个`int`是多少位的，都不可能做到等概率输出。因为当`rand()`的返回值是在$[0,2^k-1]$中随机生成的非负整数时，`n!`在`n>=3`时不是$2^k$的因子（不论`k`有多大），所以这`n!`个排列不可能是等概率的。

### 偏序关系和全序关系

在讨论完置乱问题之后，接下来讨论排序问题。在具体介绍排序算法前，首先需要界定清除，**序**（order）是一个什么东西。在上一章定义过**良序**的概念，但要对一个向量做排序，并不一定要要求它的元素是某个定义了良序关系的类型。比如说，`n`个实数同样可以关于熟知的“≤”排序。

因此，需要引入条件更松的序关系的定义。

将良序关系定义中的第4个条件（最小值）去掉，就变成了**全序**（total order）关系。
$$
\mathbf{定义（全序关系）}\\
\begin{align}
&如果集合S上的一个关系\preceq满足：\\
&   1.（\mathbf{完全性}）x\preceq y 和 y \preceq x至少有一个成立。\\
&	2.（\mathbf{传递性}）如果x\preceq y且y\preceq z，那么x\preceq z。\\
&	3.（\mathbf{反对称性}）如果x\preceq y、y\preceq x均成立，那么x=y。\\
&那么称\preceq是S上的一个\mathbf{全序关系}，同时称S为\mathbf{全序集}。
\end{align}
$$
显然良序关系是全序关系的子集。

和良序关系相比，全序关系更加符合常规的认知。比如，实数集上熟知的“≤”就是全序关系。由于**完全性**的存在，凡是具有全序关系的数据类型，都可以进行排序；反之，在《数据结构》里的<u>通常意义的排序</u>问题中，都假定数据结构中的元素数据类型具有<u>先验的全序关系</u>。

> 在C++中，排序函数`sort`接受三个参数，其中第三个参数就表示“自定义的全序关系”。基本数据类型（如`int`和`double`）定义了内置的全序关系（即熟知的“≤”），但也可以使用其他的全序关系进行排序。详见《算法设计》。
>
> 其他编程语言中的排序函数也有类似的设计。

除了全序关系之外，还有一种序关系在《数据结构》中也经常会提到：**偏序**（partial order）关系。
$$
\mathbf{定义（偏序关系）}\\
\begin{align}
&如果集合S上的一个关系\preceq满足：\\
&   1.（\mathbf{自反性}）x\preceq x。\\
&	2.（\mathbf{传递性}）如果x\preceq y且y\preceq z，那么x\preceq z。\\
&	3.（\mathbf{反对称性}）如果x\preceq y、y\preceq x均成立，那么x=y。\\
&那么称\preceq是S上的一个\mathbf{偏序关系}，同时称S为\mathbf{偏序集}。
\end{align}
$$
偏序关系和全序关系相比，第1个条件（完全性）变成了更简单的自反性；也就是说，并不是$S$中的任意两个元素都能进行比较。

> 比如，令`S`为“考生组成的集合”，$\preceq$定义为“考生`x`的<u>每一门</u>分数都小于等于考生`y`”。您可以轻易验证，这个关系是偏序关系但不是全序关系。

在计算机编程中直接定义偏序关系是不方便的，因为$\preceq$的返回值往往是`bool`类型，不存在`true`和`false`之外的第三个选项（*无法比较*）。并且，*无法比较*的情况不能随意地返回一个`true`或`false`的值，因为这可能导致传递性被破坏。

所以，当在编程时需要定义一个偏序关系时，往往会将它扩展成一个全序关系。比如，给$S$中的所有元素做标号，当已有的偏序关系*无法比较*时，则根据标号的大小进行比较。

> 扩展成全序关系之后，就可以进行排序了。由扩展成的全序关系的不同，可能会产生不同的排序结果。

### 归并排序

现在回到向量排序的问题。

对于一个线性表，如果它的数据类型是全序的；且对其中的任意一个元素`x`，和`x`的<u>后缀</u>中的任意一个元素`y`，总是有`x`$\preceq$`y`，则称它是**有序的**（ordered）。对于无序线性表，通过移动元素位置使其变为有序的过程，称为**排序**（sort）。

> 在计算机领域所说的有序，一般都是指**升序**。所以在上面的定义中使用的是“后缀”。
>
> 如果您想要讨论降序或者其他的什么顺序（比如按最小素因子排序），只需要重新定义全序关系$\preceq$，即可以回归为升序的情况进行处理。

```c++
// 问题2.9 - 向量排序
// 给定：向量V[0:n]，全序关系cmp
// 要求：按照全序关系cmp对向量V做排序
```

排序是计算机领域最重要的算法之一。在计算机出现至今，人们提出了各种各样的排序算法，并且仍然有不少研究者在从事着排序算法的研究。在《数据结构》中，将专门有一章讨论各种排序算法。在本节，先介绍一种最基本、最经典的排序方法：**归并排序**（merge sort）。

> 归并排序的发明人是大名鼎鼎的冯·诺依曼，这位“计算机之父”在1945年设计并实现了该算法。

归并排序的设计采用的仍然是递归的思想：

* 规模`n <= 1`的向量总是天然有序的。

* 对于规模`n > 1`的向量，可以将其分成前后两部分，长度分别为`n/2`和`n-n/2`（您在上一章见过这个分法），从而将规模为`n`的问题化归为两个规模较小的子问题。这些子问题可以继续递归下去直到化为`1`。解决子问题之后，`V`的前半部分和后半部分分别有序，只需要将这`2`个有序序列合并为`1`个有序序列，就可以解决原问题了。这一合并的过程就称为**归并**（merge）。

您可以根据上面的思想，自己实现一个归并排序的算法，然后和下面的示例算法进行比较。

> 这个代码比较长。最好自己先写一份代码，因为直接读示例代码很难记住。
>
> 注意，在《数据结构》部分，代码的记忆既不是重点也没有必要。归并排序这个知识点的核心是上面的这一段文字：即**归并**的思想。

```c++
// 算法2.9A
// 其中，B是一个辅助数组
template <typename T> /* ...省略中间内容，详见配套代码... */
void mergeSort(T* A, int n, function<bool(const T&, const T&)> cmp) {
    if (n <= 1) { return; }        // 递归边界
    int m = n / 2;                 // 取中点
    mergeSort(A, m, cmp);          // 递归地排序前半部分
    mergeSort(A+m, n-m, cmp);      // 递归地排序后半部分
    arrayCopy(B, A, m);            // 开始归并，将前半部分复制到辅助数组，后半无需复制
    T* A1 = B, * A2 = A+m;         // 前半部分记为A1，后半部分记为A2
    int L1 = m, L2 = n - m;        // A1和A2的长度
    int i = 0, j = 0, k = 0;
    while (j < L1 && k < L2) {     // 在两个部分的元素都没有用尽时
        if (cmp(A1[j], A2[k])) {   // 比较它们还未加入A的最小的元素
            A[i++] = A1[j++];      // 将二者的较小者加入到A中
        } else {
            A[i++] = A2[k++];
        }
    }
    while (j < A1) {               // 如果A1还有多余元素没加入A
        A[i++] = A1[j++];          // 就将剩余元素加入A
    }
}

template <typename T>
void Vector<T>::mergeSort(function<bool(const T&, const T&)> cmp) {
    MergeSort<T> sort;
    sort(_data, _size, cmp);
}
```

> 由于向量被封装了起来，这里没有直接在向量层次做递归，而是在<u>向量所基于的数组</u>层次做。这个编程技巧和《数据结构》的内容没有什么关系。

在阅读并理解上述归并排序的算法的基础上，您可以回答以下问题（其中，假定比较函数`cmp`的时间、空间复杂度都是$O(1)$的）：

1. 为什么向量的前半部分必须要复制到辅助数组去，而后半部分不需要？

   > 因为如果前半部分不复制的话，一旦后半部分比前半部分的元素小，就会把前半部分的元素覆盖掉；
   >
   > 而对后半部分而言，由于`A2[k]`就是`A[m+k]`，所以除非`A1`的元素已经全部加入`A`，否则`i`永远追不上`m+k`。因此，后半部分是不存在覆盖问题的。

2. 为什么没有讨论后半段`A2`有多余元素的情况？

   > 因为`A2`的数据没有复制出去，如果`A1`的元素已经全加入到`A`了，则`A2`剩余的元素已经在它们应该在的位置上，不需要再移动了。

3. 辅助数组`B`的长度至少是多少？并由此确定**算法2.9A**的空间复杂度。

   > 辅助数组`B`的长度至少为最大的`m`，也就是`n/2`，因此空间复杂度为$\Theta(n)$。
   >
   > 注：递归产生的$\Theta(\log n)$，相比于辅助数组的$\Theta(n)$来说可以忽略；但<u>在解答题的场合不可忽略，必须书写在解题过程中</u>。

4. **算法2.9A**的时间复杂度是多少？如果每次划分的时候`m`不取`n/2`而是取`kn`（其中`0<k<1`），时间复杂度又会变成多少？如果`m`取`max(n/2, C)`，其中`C`是一个给定的常数，那么时间复杂度又会变成多少？

   > 这个问题是归并排序相关的一个经典问题，考试中也可能出现。
   >
   > 对于原始的**算法2.9A**（**折半二分**），您可以列出$T(n)=2T(\frac n2)+\Theta(n)$的方程，递降计算出$T(n)=\Theta(n\log n)$。当`m`取`kn`（**定比二分**）时做法类似，时间复杂度不会变，但常数会增加（可以令上面方程中的$\Theta(n)=n$，以方便讨论常数）。
   >
   > 如果`m`存在上限`C`（**定长二分**），则在`n`充分大时，$T(n)=T(n-C)+T(C)+\Theta(n)=\Theta(n^2)$。
   >
   > 因此`m`必须按比例取，而不能受到某个固定值`C`的限制。

5. 如果向量已经基本有序，只有开头的长度为`L`的一小段前缀是乱序的（即前缀外全部有序，且前缀中的元素都比前缀外的元素小），如何改进**算法2.9A**，让它可以有更高的时间效率？改进之后的时间复杂度是多少？

   > 这个问题也是一个排序经典问题，考试中可能出现。
   >
   > 首先容易证明，原有的**算法2.9A**在最好情况下，时间复杂度也是$\Theta(n\log n)$的。所以必须要改进。
   >
   > 改进的时候，可以针对已知的方程$T(n)=2T(\frac n2)+\Theta(n)$做优化。这个方程中，递归项$2T(\frac n2)$只要不改动递归方式，就是没法做优化的；而余项$\Theta(n)$是有机会被优化的。需要在“<u>比较好的情况</u>”（即题中给出的“基本有序”的情况）下，让$\Theta(n)$变得更小。
   >
   > 就这个问题而言，只需要在归并前增加一行：
   >
   > ```c++
   > if (A[m] <= A[m+1]) { return; }
   > ```
   >
   > 这样，如果归并前的序列已经有序，就不需要进行归并。
   >
   > 那么，不需要归并的部分就可以降到$T(n) = 2T(\frac n2) + O(1)$，也就是$\Theta(n)$。
   >
   > 需要归并的部分由题意，长度不超过`2L`，在这部分利用上一题中获得的结论，就可以得到时间复杂度为$\Theta(L\log L)$。
   >
   > 因此，改进后的时间复杂度为$\Theta(n+L\log L)$。

上述问题中4、5两题是典型的排序问题的命题角度。在后面的章节中介绍到其他的排序时，还将再次从这个角度进行观察。

### 基于比较的排序的时间复杂度

归并排序是一种**基于比较**（comparison-based）的排序。

> 所谓基于比较，就是在算法进行过程的每一步，都依赖于元素的比较（即调用`cmp`）进行。
>
> 基于比较的排序是针对**全序关系**设计的。大多数的排序算法都是基于比较的。
>
> 还有一些不基于比较的排序，它们不是针对待排序数据类型的**全序性**设计的，而是针对待排序数据类型的其他性质设计的，因而应用范围会更小。在后文中会介绍一些不基于比较的排序。

下面将证明一个重要结论：
$$
\mathbf{定理}\\
基于比较的排序，在最坏情况下的时间复杂度必定是Ω(n\log n)。
$$
这是本笔记中第一次使用信息论方法，讨论时间复杂度的**最优性**。信息论方法在考研试卷上通常不会直接考到，但用信息论的思路，有助于在算法设计题中判断自己是否能得到时间复杂度层面上的满分。

* 在排序算法开始之前，这`n`个元素可能的顺序关系有`n!`种，而在排序算法开始之后，这`n`个元素可能的顺序关系只有`1`种（因为已经找到了它们的顺序）。

  > 在最坏情况下，`n`个元素互不相同，排序后的顺序关系是确定的。

* 另一方面，每次比较都有两种结果（`if`分支和`else`分支）。剩下的可能的顺序关系被分为`2`个部分，根据比较结果，只保留其中的`1`个部分。

  > 在最坏情况下，每次保留的都是元素较多的部分，从而每次比较至多排除一半的可能。

综合以上两点，至少需要进行$\log_2(n!)=\Theta(n\log n)$次比较。

这个结论基于一个重要的公式：
$$
\mathbf{定理（Stirling公式）}\\
在n\to\infty时，n!\sim \sqrt{2\pi n}\cdot \left(\frac ne\right)^n。
$$

> 这一公式的证明和计算机考研无关。感兴趣的话可自行在网上查找，这里不再叙述。
>
> 在《数据结构》的学习中需要记住的公式并不多，Stirling公式是必须记住的公式之一。或者，您也可以只记住$\log(n!)=\Theta(n\log n)$，因为这是Stirling公式在《数据结构》考研中的主要应用。

由此可见，归并排序在基于比较的排序中，已经达到了最优的时间复杂度。当然，空间复杂度不是最优的，它需要$\Theta(n)$的额外空间。关于排序的更多性质，在后面的专门章节中将继续分析。

### 折半查找

在介绍完排序之后，来看一下排序之后得到的有序向量，在查找时有什么额外的优越性。

执行查找操作的时候，不再需要一个一个元素看是否相等了。这里可以使用刚才介绍的基于比较的算法思路。

将被查找的元素`e`和向量中的某个元素`V[i]`比较，比较结果有2种：

* 如果`V[i] > e`，那么只需要保留`V[0:i]`作为新的查找区间；
* 如果`V[i] <= e`，那么只需要保留`V[i:n]`作为新的查找区间。

当取`i = n/2`（**折半二分**）时，可以保证新的查找区间长度大约是原来的一半。所以这个思路称为**折半查找**。您可以尝试设计折半查找的算法。

```c++
// 问题2.10 - 向量查找
// 给定：向量V[0:n]，全序关系cmp
// 输入：待查找的元素e
// 输出：元素e在向量V中的秩
//      如果有多个满足条件的元素，输出最大的秩
//      如果e在V中不存在，输出-1
```

> 查找是算法设计题的考试重点，在设计的时候，需要尤其注意多个相等元素的时候是返回秩最大、秩最小还是任意一个，以及查找失败的时候返回何种特殊值。
>
> 上面的问题描述中，最后两行就是针对“多个结果”和“没有结果”情况的输出规定。

和归并排序一样，笔者建议您首先自己写一个答案。

```c++
// 算法2.10A
template <typename T> /* ...省略中间内容，详见配套代码... */
int binarySearch(T* A, int n, T e, function<bool(const T&, const T&)> cmp) {
    int i = n / 2;                   // 折半
    if (n == 1) {                    // 递归边界
        return A[0] == e ? 0 : -1;
    }
    if (!cmp(A[i], e)) {
        return binarySearch(A, i, e);      // 递归进入左半部分
    } else {
        int r = binarySearch(A+i, n-i, e); // 递归进入右半部分
        return r < 0 ? r : r + i;    // 复原在整个数组中的下标
    }
}

template <typename T>
Rank Vector<T>::binarySearch(T e, function<bool(const T&, const T&)> cmp) {
    BinarySearch<T> search;
    return search(_data, _size, e, cmp);
}
```

**算法2.10A**的时间复杂度和空间复杂度均为最坏$\Theta(\log n)$，请您自己证明这一点。

### 消除简单尾递归

查找`V[0:n]`中某个元素`e`的下标，这个问题在计算前有`n+1`种（包括`-1`）可能的结果，计算后有`1`种确定的答案，因此最坏时间复杂度一定是$\Omega(\log n)$的。但空间复杂度并不一定要是$\Omega (\log n)$。在这一小节，将介绍一种叫做**消除尾递归**的技术，使用这个技术，可以将**算法2.10A**的空间复杂度降为$O(1)$。

如果一个递归函数只在**返回**（return）前调用自身，则称其为**尾递归**（tail recursion）。特别地，如果在返回前只调用自身至多一次，则称为**简单尾递归**。在本小节，只介绍对于简单尾递归的消除方法。

对于简单尾递归，只需要将递归函数的参数作为循环变量，就可以将其改写为不含递归的形式，从而降低空间复杂度。

```c++
// 包含尾递归的原函数
ReturnType Function(ParameterType Parameter) {
    if (Condition(Parameter)) { // 递归边界
        return BoundaryValue(Parameter);
    }
    // ...（函数体）
    return Function(NextParameter(Parameter)); // 尾递归
}
```

上面是一个简单尾递归的一般模型。将递归参数`Parameter`改为循环变量，则可以变成：

```c++
// 不含尾递归的新函数
ReturnType Function(ParameterType Parameter) {
    ParameterType p = Parameter;
    while (!Condition(p)) {
        // ...（函数体）
        p = NextParameter(p); // 被消除的尾递归
    }
    return BoundaryValue(p);
}
```

这个方法在**算法2.10A**上不能直接应用，因为返回时可能会加上一个偏置量（`i`）。不过，因为加法具有结合律，所以只需要增加一个循环变量维护<u>累计偏置量</u>就可以了。您可以自己利用上述方法来尝试将**算法2.10A**的空间复杂度优化到$O(1)$，下面是一个改写后的例子。

```c++
// 算法2.10B
int binarySearch(T* A, int n, T e, function<bool(const T&, const T&)> cmp) {
    int i = 0;                 // 维护累计偏置量
    while (n > 1) {            // 判断是否到达递归边界
        int m = n / 2;         // 折半
        if (!cmp(A[m], e)) {
            n = m;             // 递归进入左半部分
        } else {
            i += m;
            A += m;            // 递归进入右半部分
            n -= m;
        }
    }
    return A[0] == e ? i : -1; // 递归边界
}
```

> 当“没有结果”的时候，返回无效秩`-1`是一个比较自然的想法。
>
> 但是有的时候需要返回一个其他的数值。比如说，要向有序向量`V`中插入一个新元素`e`，但不希望破坏它的有序性。那么，就必须找到一个<u>合理的插入位置</u>。
>
> 在**算法2.10B**中，最后一行改为直接`return i`，那么就可以返回到一个合理的插入位置：新元素`e`一定应当被插入在`V[i]`<u>之前或之后</u>。请注意**算法2.10B**直接进行这样的修改，并不能判断出应当插入在之前还是之后，还需要对`V[i]`和`e`的大小进行比较；您可以做出适当的调整，让它能够直接返回合理的插入位置。

现代编译器通常可以在编译的过程中自动消除简单尾递归，所以在实际上机编程时，不需要刻意将简单尾递归改写成循环形式。但在《数据结构》学科分析算法的时候，不应该考虑编译器做的优化，所以在算法设计题中，如果出现了未被改写的简单尾递归，几乎一定是会被扣分的。

### 向量唯一化

下面讨论一下唯一化问题。

```c++
// 问题2.11 - 向量唯一化
// 给定：向量V[0:n]
// 要求：将V中的重复元素删去，只保留秩最小的那一个
```

一个简单但有效的解法是：从左到右考察`V`中的每个元素，如果它在自己的前缀中已经出现过了，那么就把这个元素删除。同样，在看下面的代码前，您可以自己实现这个算法。

```c++
// 算法2.11A
template <typename T>
void Vector<T>::duplicate() {
    Rank i = 1, k = 1;                  // 快指针i检索，慢指针k填充
    while (i < _size) {                 // V[0:k]始终是V[0:i]唯一化后的结果
        bool existInPrefix = false;     // 要判断V[i]是否在V[0:i]中
        for (Rank j = 0; j < k; ++j) {  // 只需要判断是否在V[0:k]中即可
            if (_data[i] == _data[j]) {
                existInPrefix = true; break;
            }
        }
        if (existInPrefix) {            // 如果V[i]不在它的前缀中
            _data[k++] = _data[i++];    // 移动元素，并同时移动快慢指针
        } else { ++i; }                 // 否则只需要移动快指针
    }
    _size = k;                          // 修改规模，和慢指针对齐
    shrink();                           // 如果有必要，则缩容
}
```

最坏情况是所有`V`中的元素互不相等的情况。为了验证所有元素互不相等，必须要进行至少$\Theta(n^2)$次比较。所以**算法2.11A**的最坏时间复杂度是$\Theta(n^2)$。

而最好情况是所有`V`中元素全部相等的情况，最好时间复杂度是$\Theta(n)$。

> 这个最好时间复杂度得益于在删除元素时使用了**算法2.7C**引入的快慢指针。如果像**算法2.7A**和**算法2.7B**一样每次都使用`remove`进行删除，那么最好情况下也需要$\Theta(n^2)$的时间。

在有序向量的情况下，相等的元素总是排在连续的位置的。所以，要讨论一个元素是否在它的前缀中可以找到，只需要比较它是否和自己的直接前驱相等即可。

根据这一思路，可以对**算法2.11A**进行改进：只需要将`existInPrefix`的计算过程简化为`existInPrefix = _data[i] == _data[k-1]`。改进后的算法，时间复杂度是$\Theta(n)$。

> 如果定义了全序关系（即可排序），那么无序向量的唯一化可以化归到有序向量的情况进行处理：先进行一次$\Theta(n\log n)$的排序，再用有序向量唯一化。但在排序的时候，会损失“元素原先的位置”这一信息，所以需要开辟一个额外的$\Theta(n)$的空间保存这一信息，以在唯一化之后能够顺利还原。

### 循环位移

在本节的最后，用**循环位移**（cyclic shift）作为例子，讨论一下非常规的遍历方法。

给定向量`V[0:n]`和位移量`k`，则将原有的向量`V[0],V[1],...,V[n-1]`，变换为`V[k],V[k+1],...,V[n-1],V[0],V[1],...,V[k-1]`，称为**循环左移**。相应地，变换为`V[n-k],V[n-k+1],...,V[n-1],V[0],V[1],...,V[n-k-1]`，称为**循环右移**。循环左移和循环右移的实现方法大同小异，这一小节只讨论循环左移，右移的情况请您自己完成。

```c++
// 问题2.12 - 循环位移
// 给定：向量V[0:n]，位移量k（0<k<n）
// 要求：将向量V循环左移k个单位
```

相信您一定知道最经典的**交换**函数`swap`的实现：

```c++
t = a, a = b, b = t;
```

一个最朴素的想法，就是用类似的辅助空间，暂存`V[0:k]`中的元素，然后通过3次赋值（实际上是数组拷贝）来完成循环左移。

```c++
// 算法2.12A
template<typename T>
void cyclicLeftShiftA(const Vector<T>& V, int k) {
    T* B = new T[k], * A = V.data();
    int n = V.size();
    arrayCopy(B, A, k);     // B[0:k] = V[0:k]
    arrayCopy(A, A+k, n-k); // V[0:n-k] = V[k:n]
    arrayCopy(A+n-k, B, k); // V[n-k:n] = B[0:k]
}
```

这一算法的时间复杂度是$\Theta(k)+\Theta(n-k)+\Theta(k)=\Theta(n+k)$。考虑到$k=O(n)$，也可以简化为$\Theta(n)$。空间复杂度则为$\Theta(k)$。

下面的目标则是将空间复杂度降到$O(1)$。

为了保持时间复杂度仍然为$\Theta(n)$不变，需要尽可能“一步到位”地移动元素。现在让`V[i+k]`移动到`V[i]`的位置上，暂存`V[i]`到辅助空间`temp`。下一步，如果继续将`V[i+k+1]`移动到`V[i+1]`的位置，那么需要的辅助空间就会增大。为了防止辅助空间增大，则需要考虑“不用暂存”的元素：也就是已经被移动的`V[i+k]`。下一步将`V[i+2k]`移动到`V[i+k]`的位置，这就是不需要新的辅助空间的。

因此可以得到一个算法：将`V[i+k]`移动到`V[i]`，再将`V[i+2k]`移动到`V[i+k]`，以此类推。由于向量中的元素是有限的，您可以证明，存在`j`，使得`(i+jk) % n = i`，也就是经过`j`次移动后回到了`V[i]`。最后一次赋值，将辅助空间里的`V[i]`拿出来赋给`V[i-k]`也就是`V[i+(j-1)k]`即可。

需要注意的是，这样一轮并不一定能经过`V`中所有的元素。比如在`n=6, k=2, i=0`时，只轮转交换了`V[0],V[2],V[4]`这3个元素，而对另外3个元素则没有移动。

下面证明：对任意的秩`0 <= r < n `，都存在唯一的`0 <= i < d`和`0 <= j < n/d`，使得`r = (i+jk) % n`。其中，`d = gcd(n,k)`是`n`和`k`的最大公约数。

> 因为`r`和`(i,j)`的取值范围都是`n`元集，所以只要证明`(i,j)`到`r`是单射，就蕴含了它同时是满射。
>
> 因此，只需要证明对于不等的$(i_1,j_1)$和$(i_2,j_2)$，$(i_1+j_1k) \%n\ne(i_2+j_2k)\%n$。
>
> 假设存在整数$q$，使得$(i_1-i_2)+(j_1-j_2)k+qn=0$。由于$(j_1-j_2)k+qn$必定是$d$的倍数，而$|i_1-i_2|<d$，所以只能有$i_1=i_2$。
>
> 设$k=k_1d,n=n_1d$，那么$(j_1-j_2)k_1+qn_1=0$。因为$(k_1,n_1)=1$，所以$j_1-j_2$必定是$n_1$的倍数，但$|j_1-j_2|<n/d=n_1$，所以只能有$j_1=j_2$。这和$(i_1,j_1)$与$(i_2,j_2)$不等矛盾，故由反证法得到单射成立。

于是，遍历顺序应当是：依次从`0`、`1`、……、`d-1`出发，以`k`为步长遍历`n/d`次回到起点。

```c++
// 算法2.12B
template<typename T>
void cyclicRightShiftB(const Vector<T>& V, int k) {
    auto A = V.data(), n = V.size();
    int d = gcd(n, k), n1 = n / d;      // 计算最大公约数
    T temp;                             // 辅助空间
    for (int i = 0; i < d; ++i) {       // 外层循环
        temp = A[i];                    // 放入辅助空间
        for (int j = 0; j < n1-1; ++j) {  // 内层循环，以k为步长位移
            A[(i+j*k) % n] = A[(i+(j+1)*k) % n];
        }
        A[(i+(n1-1)*k) % n] = temp;       // 最后一步，从辅助空间取出
    }
}
```

> 为了和上面的分析保持一致，这里直接用`(i,j)`作为循环变量。
>
> 实际上，可以用`(i,i+j*k)`作为循环变量，以降低`j*k`乘法运算的次数。您可以自己改写这个算法。

这一算法的时间复杂度是$\Theta(d)\cdot \Theta(n/d) = \Theta(n)$，空间复杂度降低到了$O(1)$。

在很多教材上会介绍另一种解法。

```c++
// 算法2.12C
template <typename T>
void cyclicRightShiftC(const Vector<T>& V, int k) {
    auto A = V.data(), n = V.size();
    reverse(A, k);   // -> rV[0:k] + V[k:n]
    reverse(A, n);   // -> rV[k:n] + V[0:k]
    reverse(A, n-k); // -> V[k:n] + V[0:k]
}
```

其中`reverse`表示将数组颠倒过来。

```c++
template <typename T>
void reverse(T* A, int n) {
    for (int i = 0; i < n/2; ++i) {
        swap(A[i],A[n-1-i]);
    }
}
```

容易证明，**算法2.12C**和**算法2.12B**具有相同的时间复杂度和空间复杂度。但是，**算法2.12B**的赋值次数是$d(n_1+1)=n+d\le \frac 32 n$，而**算法2.12C**的赋值次数是$\frac32 \cdot(k+n+(n-k))=3n$，所以**算法2.12B**的常数是比**算法2.12C**更小的。

> 这里只讨论数组元素的赋值，而不讨论循环变量`i`等的赋值。
>
> 这是因为在计算机中，循环变量总是存在寄存器里，而数组元素存在内存中，二者的读写速度相差很大。关于这个问题，详见《组成原理》部分。



